---
title: 'Word Vectors'
description: 'Natural Language Process'
---

import BackgroundText from '@content/components/BackgroundText'

# Tokenization

우리는 문장을 이해하기 위해 문장을 구성하는 각각의 단어들을 봅니다.
여러 단어들의 구성을 보고 문장의 의미를 찾을 수 있는 것이죠.
컴퓨터도 이와 마찬가지로 문장을 더 작은 단위, 토큰으로 쪼개어 바라볼 것입니다.
그리고 이러한 과정을 우리는 <BackgroundText>토큰화(Tokenize)</BackgroundText> 라고 불러요.

우리는 문장을 볼 때 각각의 단어만을 보는 것이 아닌 쉼표, 온점, 따옴표 등 여러가지 특수 문자도 함께 보고 있어요.
특수 문자에 따라 어떠한 방식으로 해석할 지 달라질 수 있기 때문이죠.
또한 모르는 단어라고 하더라도 pre-, post- 와 같은 접두사와 접미사를 통해 어느정도 예측을 할 수 있죠.
토큰 역시 마찬가지 입니다.
우리는 각 단어 단어를 토큰으로 만드는 것이 아닌 더 작은 의미로 토큰을 생성할 수 있습니다.
토큰화를 하는 방식은 여러가지가 있기 때문에 토큰 = 단어 라는 생각을 가지면 안돼요!

* 먼저 단어 마다 토큰화를 할 수 있어요. <BackgroundText>(coarse-grained)</BackgroundText>   
아주 직관적이지만 vocab의 사이즈가 커지게 되고, 새로운 단어가 생기거나 할 때 vocab이 변화할 가능성이 있죠.  
* 또 철자를 기준으로 나눌 수 있겠죠. <BackgroundText>(fine-grained)</BackgroundText>  
vocab의 사이즈는 작고 변화할 일이 없겠지만, 그 의미를 거의 담을 수 없겠죠.  
* 위 두 가지를 절충하는 방법을 사용할 수도 있어요. <BackgroundText>(subword-based)</BackgroundText>  

![](assets/word_vectors/01.JPG)




## BPE(Byte Pair Encoding)

우리는 토큰화를 진행할 때, 단어 집합에 없는 단어가 생성되면 [Unkwon Token]을 생산하게 됩니다.
이를 <BackgroundText>OOV (Out Of Vocabulary)</BackgroundText> 문제라고 해요.
그래서 단어를 의미있는 단위로 나누는 작업을 진행하려고 합니다.
대표적인 subword 분리 알고리즘 BPE에 대해 알아봅시다.

``
aaabdaaabac
``
위와 같은 문장이 있다고 합시다.
가장 자주 등장하는 byte pair에 대해 치환하는 방식을 적용해볼 거예요.
aa를 Z로 바꿔서 표기해보죠.
``
ZabdZabac
Z=aa
``
그 다음으로 자주 등장하는 byte pair에 대해 치환을 한 후 또 적용해볼 거예요.
```
ZYdZYac
Z=aa
Y=ac
```
byte pair가 없어질 때 까지 계속 진행할까요?
```
XdXac
Z=aa
Y=ac
X=ZY
```
이제 더 압축할 수 있는 byte pair는 없어 보여요.
이렇게 문자를 압축하는 형태로 만드는 알고리즘이 BPE 입니다.
이를 자연어 처리에서 활용하는 예시를 보도록 하죠.

```
# dictionary
l o w : 5,  l o w e r : 2,  n e w e s t : 6,  w i d e s t : 3

```
각 단어가 위와 같은 빈도수로 출현했다고 생각을 해 봅시다.
이를 참고하여 만든 vocab의 초기 집합은 아래와 같아요.
```
# vocabulary
l, o, w, e, r, n, s, t, i, d
```
BPE 알고리즘은 몇번 반복할지 사용자가 지정해줄 수 있습니다.
우리는 총 10번을 반복하여 subword를 만들어 낸다고 생각을 해보죠.
```
# dictionary update!
l o w : 5,
l o w e r : 2,
n e w es t : 6,
w i d es t : 3
# vocabulary update!
l, o, w, e, r, n, s, t, i, d, es
```
먼저 가장 빈도 수가 높은 e, s 를 하나로 묶어 표현을 해 줄 것입니다.
```
# dictionary update!
l o w : 5,
l o w e r : 2,
n e w est : 6,
w i d est : 3
# vocabulary update!
l, o, w, e, r, n, s, t, i, d, es, est
```
다음으로 빈도 수가 높은 es, t 를 하나로 묶어 표현합니다.
이러한 방식으로 총 10번을 반복하여 얻은 결과는 아래와 같아요.
```
# dictionary update!
low : 5,
low e r : 2,
newest : 6,
widest : 3
# vocabulary update!
l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest
```

이제 우리는 lowest라는 단어를 low와 est의 형태로 인코딩할 수 있게 되었어요.