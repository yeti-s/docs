---
title: 'Summarization Module'
description: 'Summarize news content'
---

낚시성 제목은 가진 기사의 내용을 통해 새로운 제목을 생성해주는 요약 모델 개발을 담당하였다.

# Base 모델 선정

높은 한국어 인식율을 위해 문서 요약 모델로 채택된 BART의 한국어 문장으로 pre-trained된 모델을 찾아보았다.  
[SKT-AI KoBART](https://github.com/SKT-AI/KoBART)

SKT에서 제공하는 KoBART가 있어 해당 github에서 제시하는 dependency를 설치를 위해 새로운 conda 환경을 추가하기 싫어 다른 방법을 찾아보았다.
찾아보니 huggingface hub [gogamza/kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2)로 동일한 모델이 이미 올라온 듯 하여 이를 사용하기로 했다. 

```python
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
model_name = 'gogamza/kobart-base-v2'

# get model, tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

# Dataset

데이터셋은 AI-Hub에서 제공하는 [문서요약 텍스트](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=data&dataSetSn=97)를 사용하기로 하였다.  
위 데이터셋은 법률, 사설, 그리고 신문기사 총 세 가지 범주로 나뉘어 있는데, 우리의 관심 영역은 뉴스의 요약이기 때문에 뉴스 카테고리에 대한 데이터만 사용하도록 한다.

기본적으로 제목, 본문, 요약 뿐 아니라 불용어 위치 정보, 미디어 명, 신뢰성과 같은 기사의 메타 데이터를 포함하고 있는데 이를 모델 학습에 어떻게 사용할 수 있을까 분석하는 시간이 필요할 것 같다.
일단 기본적인 요약 작업 테스트를 위해 본문의 내용과 요약된 라벨 데이터를 사용하여 학습을 진행하도록 하겠다.

매번 학습을 시작할 때 마다 json 데이터를 읽고 여러 전처리 과정을 거치는 것이 생각보다 시간이 오래 걸려 처리된 데이터를 저장하고 학습 마다 불러오는 것으로 설계하였다.
```python
import json
from threading import Thread
from datasets import Dataset

creates_dataset = True
dataset_file = '../data/dataset1.pt'
train_file = '../data/train_original.json' # num of total data is about 240000
valid_file = '../data/valid_original.json' # num of total data is about 30000
num_threads = 8

# read json & tokenize
def get_input_and_labels(documents, articles, abstractives):
    for document in documents:
        article = ''
        for text in document['text']:
            if len(text) > 0:
                article += (text[0]['sentence'] + ' ')
        articles.append(article)
        
        abstractive = document['abstractive']
        if len(abstractive) > 0:
            abstractive = abstractive[0]
        abstractives.append(abstractive)
        
def get_dataset_from_json(json_file, num_data=0):
    with open(json_file, 'r') as f:
        json_data = json.load(f)
        documents = json_data['documents']
        data_size = len(documents)
        if num_data == 0 or num_data > data_size:
            num_data = data_size
        
        data_per_threads = num_data//num_threads
        t_results = []
        threads = []
        for i in range(num_threads-1):
            t_result = [[], []]
            t_results.append(t_result)
            
            thread = Thread(target=get_input_and_labels, args=(documents[i*data_per_threads:(i+1)*data_per_threads], t_result[0], t_result[1],))
            thread.daemon = True
            thread.start()
            threads.append(thread)
            
        data_dict = {'article':[], 'abstractive':[]}
        get_input_and_labels(documents[(num_threads-1)*data_per_threads:], data_dict['article'], data_dict['abstractive'])

        for thread in threads:
            thread.join()
        
        for t_result in t_results:
            data_dict['article'].extend(t_result[0])
            data_dict['abstractive'].extend(t_result[1])
            
        return Dataset.from_dict(data_dict)

if creates_dataset:
    train_dataset = get_dataset_from_json(train_file)
    val_dataset = get_dataset_from_json(valid_file)
```
batch 작업을 위해 모든 input을 동일한 BART 최대 길이인 1024로 설정해 주었다.
input을 최대 길이로 padding 해주었기 때문에 계산 효율을 위해 attention_mask 역시 활용.  
label은 loss 계산 당시 동일한 input과 동일한 길이가 아니면 오류가 발생해서 똑같이 1024로 설정 해주었다.
```python
from torch.utils.data import DataLoader, TensorDataset

batch_size = 4

def preprocess(examples):
    inputs = tokenizer(examples['article'], return_tensors='pt', max_length=1024, padding='max_length', truncation=True)
    labels = tokenizer(examples['abstractive'], return_tensors='pt', max_length=1024, padding='max_length', truncation=True)
    inputs['labels'] = labels['input_ids']
    return inputs

def create_dataloader(dataset):
    input_ids = dataset['input_ids']
    attention_mask = dataset['attention_mask']
    labels = dataset['labels']
    tensor_dataset = TensorDataset(input_ids, attention_mask, labels)
    return DataLoader(tensor_dataset, batch_size=batch_size)

if creates_dataset:
    dataloader = {
        'train': create_dataloader(train_dataset.map(preprocess, batched=True).with_format("torch")),
        'val': create_dataloader(val_dataset.map(preprocess, batched=True).with_format("torch"))
    }
    torch.save(dataloader, dataset_file)
else:
    dataloader = torch.load(dataset_file)
```

# Train

validation dataset 자체도 약 30,000개 데이터로 이루어져 있어 평가 시간이 오래 걸리지만 더 정확한 측정을 위해 sampling은 하지 않는 것으로 하였다.
```python
# evaluate model
from tqdm import tqdm

@torch.no_grad()
def eval_model(model, val_dataloader):
    device = next(model.parameters()).device
    model.to(device)
    model.eval()
    total_loss = 0
    
    print('=== evaluate model')
    for _, data in enumerate(tqdm(val_dataloader)):
        data = [t.to(device) for t in data]
        inputs = {
            'input_ids': data[0],
            'attention_mask': data[1],
            'labels': data[2]
        }
        outputs =  model(**inputs)
        loss = outputs.loss
        total_loss += loss.item()
    
    total_loss /= len(val_dataloader)
    print(f'total loss : {total_loss}')
    
    return total_loss
```

학습에는 epoch마다 선형적으로 learning rate를 감소시키는 scheduler를 사용하였다.  
1 사이클 학습 하는데 하루가 걸려서 checkpoint를 만들 필요가 있었다.
또한 `yolov8` 모델에서 본 전략인데, 1 사이클마다 평가를 진행하고 가장 높은 점수를 받은 데이터를 best로 따로 저장해두는 전략을 사용 할 것이다.  
그리고 학습된 모델 weight를 직접 파일로 전달하지 않고 hub를 통해 접근 가능하도록 1 사이클이 끝날 때 마다 huggingface에 push하는 작업을 추가한다.

```python
# train model
import os
from torch.optim import AdamW, lr_scheduler

class Checkpoint():
    def __init__(self, model, optimizer, scheduler) -> None:
        self.model = model
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.epoch = 0
        self.last_step = -1
        self.best_loss = 1e20
        
    def set_root_dir(self, root_dir):
        if root_dir is not None:
            self.root_dir = root_dir
            self.path = os.path.join(root_dir, 'checkpoint.pt')
            
            if not os.path.exists(root_dir):
                os.makedirs(root_dir)
                
            if os.path.exists(self.path):
                self.load(self.path)
    
    def load(self, save_path):
        checkpoint = torch.load(save_path)
        self.model.load_state_dict(checkpoint['model'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.scheduler.load_state_dict(checkpoint['scheduler'])
        self.epoch = checkpoint['epoch']
        self.last_step = checkpoint['last_step']
        self.best_loss = checkpoint['best_loss']
    
    def save(self):
        if not self.path is None:
            torch.save({
                'model' : self.model.state_dict(),
                'optimizer' : self.optimizer.state_dict(),
                'scheduler' : self.scheduler.state_dict(),
                'epoch' : self.epoch,
                'last_step' : self.last_step,
                'best_loss' : self.best_loss
            }, self.path)
        
    def step(self):
        self.optimizer.step()
        self.last_step += 1
    
    def eval(self, val_dataloader):
        if not self.root_dir is None:
            loss = eval_model(self.model, val_dataloader)
            if self.loss > loss:
                self.loss = loss
                torch.save(self.model.state_dict(), os.path.join(self.root_dir, 'best.pt'))
    
    def next(self):
        self.scheduler.step()
        self.epoch += 1
        self.last_step = -1
        self.save()
        self.model.push_to_hub('yeti-s/kobart-base-v2-news-summarization', token=WRITE_TOKEN)
        
    def close(self):
        if not self.path is None and os.path.exists(self.path):
            os.remove(self.path)


def train_model(model, dataloader, checkpoint_dir=None, epochs=1, lr=2e-5, device=torch.device('cuda')):
    model.to(device)
    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch:0.95**epoch)
    checkpoint = Checkpoint(model, optimizer, scheduler)
    checkpoint.set_root_dir(checkpoint_dir)

    for epoch in range(checkpoint.epoch, epochs):
        print(f'=== train model {epoch}/{epochs}')
        model.train()
        num_trained = 0
        total_loss = 0
        
        for step, data in enumerate(tqdm(dataloader['train'])):
            if step <= checkpoint.last_step:
                continue
            
            data = [t.to(device) for t in data]
            inputs = {
                'input_ids': data[0],
                'attention_mask': data[1],
                'labels': data[2]
            }

            # get loss
            optimizer.zero_grad()
            outputs =  model(**inputs)
            loss = outputs.loss
            total_loss += loss.item()
            
            loss.backward()
            checkpoint.step()
            num_trained += 1
            
            # save checkpoint 
            if (step+1) % 1000 == 0:
                checkpoint.save()
                print(f'loss : {total_loss/num_trained}')
        
        checkpoint.eval(dataloader['val'])
        checkpoint.next()
        
    # remove checkpoint
    checkpoint.close()
```

