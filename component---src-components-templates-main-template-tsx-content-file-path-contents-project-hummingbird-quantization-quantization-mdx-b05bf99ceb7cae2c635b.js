"use strict";(self.webpackChunkyeti_docs=self.webpackChunkyeti_docs||[]).push([[3665],{1620:function(n,e,t){t.r(e),t.d(e,{default:function(){return S}});var r=t(1151),a=t(7294);t(2439);function o(n){const e=Object.assign({p:"p",h1:"h1",pre:"pre",code:"code"},(0,r.ah)(),n.components);return a.createElement(a.Fragment,null,a.createElement(e.p,null,"프로젝트에서 사용될 언어 모델의 규모가 너무 커"),"\n",a.createElement(e.p,null,"기존에 연구하기 위해 사용해본 torch.quantization 모듈은 추론시 GPU를 사용할 수 없다는 단점이 있었습니다.\r\n이를 해결하기 위해 ONNX를 활용할 수 있을 것입니다.\r\n먼저 Pytorch를 활용하여 Activation과 Weight에 대한 Scale을 진행하는 Layer를 생성하고, ONNX로 변환하여 Quantization을 진행합니다.\r\n그리고 GPU로 불러와서 추론을 진행하는 방식으로 해결해볼 생각입니다."),"\n",a.createElement(e.h1,{id:"onnx"},"ONNX"),"\n",a.createElement(e.p,null,"어쩌면 ONNX에서 모듈을 수정할 수 있을 것 같아서 먼저 ONNX가 무엇인지, Pytorch와는 어떤 관계인지 알고 가야할 것 같습니다."),"\n",a.createElement(e.h1,{id:"onnx-quantization"},"ONNX Quantization"),"\n",a.createElement(e.p,null,"위 방법을 실행하기 전에 Scaling을 진행하는 레이어에 대해서는 Quantization이 진행되지 않아야 하는 것을 확인해야 합니다.\r\n또 pytorch 모델을 onnx로 변환하고 이를 Quantization 하여 추론하는 방법을 알아야 하죠.\r\n간단한 모델을 활용해서 테스트를 해보자구요."),"\n",a.createElement(e.pre,null,a.createElement(e.code,{className:"language-python"},"import torch\r\nimport torch.nn as nn\r\n\r\nclass CustomModule(nn.Module):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        self.register_buffer('scale', torch.tensor(100))\r\n        self.fc = nn.Linear(1024, 1)\r\n    \r\n    def forward(self, x):\r\n        x = x * self.scale\r\n        return self.fc(x)\r\n        \r\ncustom = CustomModule()\r\ndummy_input = torch.rand((1, 1024))\r\ncustom(dummy_input)\n")),"\n",a.createElement(e.pre,null,a.createElement(e.code,null,"tensor([[-42.1489]], grad_fn=<AddmmBackward0>)\n")),"\n",a.createElement(e.p,null,"먼저 SmoothQuant의 레이어와 같이 입력에 Scaling을 적용한 후 연산을 진행하는 방법으로 간단한 모델을 구현해봤습니다.\r\nDummy 입력을 모델에 넣었더니, 52.9578 이라는 결과를 얻을 수 있었습니다."),"\n",a.createElement(e.pre,null,a.createElement(e.code,{className:"language-python"},"import onnxruntime as ort\r\nimport numpy as np\r\n\r\ninput_names = ['x']\r\noutput_names = ['y']\r\ntorch.onnx.export(\r\n    custom,\r\n    dummy_input,\r\n    verbose=True,\r\n    f='tmp/custom_model.onnx',\r\n    input_names=input_names,\r\n    output_names=output_names,\r\n)\r\n\r\ndef infer_onnx_model(model_path):\r\n    session = ort.InferenceSession(model_path)\r\n    input_name = session.get_inputs()[0].name\r\n    return session.run(None, {input_name: dummy_input.numpy()})\r\n\r\ninfer_onnx_model('tmp/custom_model.onnx')\n")),"\n",a.createElement(e.pre,null,a.createElement(e.code,null,"[array([[-42.14892]], dtype=float32)]\n")),"\n",a.createElement(e.p,null,"이제 이 모델을 ONNX로 변환하였습니다.\r\n변환시 입력의 크기를 알기 위해 Dummy Input을 요구하더라구요.\r\n이후 변환된 ONNX 파일을 불러와 추론을 하였습니다.\r\nPyTorch 모델과 같은 결과를 얻는 것을 볼 수 있었죠."),"\n",a.createElement(e.pre,null,a.createElement(e.code,{className:"language-python"},"from onnxruntime.quantization import quantize_dynamic\r\nquantize_dynamic('tmp/custom_model.onnx', 'tmp/q_custom_model.onnx')\r\ninfer_onnx_model('tmp/q_custom_model.onnx')\n")),"\n",a.createElement(e.pre,null,a.createElement(e.code,null,"[array([[-40.4947]], dtype=float32)]\n")),"\n",a.createElement(e.p,null,"Dynamic Quantization을 사용하여 모델을 변환하고 이를 같은 입력에 대해 추론을 진행하였습니다.\r\n기존 결과와 약간 달라지는 것을 볼 수 있었죠.\r\n아무튼 이런 방식으로 Quantization을 적용할 수 있겠네요."),"\n",a.createElement(e.p,null,"그런데 과연 우리가 원하는 대로 Non-Learnable Parameter에 대해서도 양자화가 진행되는지 확인할 필요가 있을 것 같습니다.\r\n그래야 SmoothQuant를 적용할 수 있으니까요."),"\n",a.createElement(e.pre,null,a.createElement(e.code,{className:"language-python"},"import onnx\r\n\r\nonnx_model = onnx.load('tmp/q_custom_model.onnx')\r\ngraph_def = onnx_model.graph\r\n\r\nfor node in graph_def.node:\r\n    print('name:', node.name)\r\n    print('op_type:', node.op_type)\r\n    print('input:', node.input)\r\n    print('output:', node.output)\n")),"\n",a.createElement(e.pre,null,a.createElement(e.code,null,"name: /Mul\r\nop_type: Mul\r\ninput: ['x', 'onnx::Mul_7']\r\noutput: ['/Mul_output_0']\r\n\r\nname: /Mul_output_0_QuantizeLinear\r\nop_type: DynamicQuantizeLinear\r\ninput: ['/Mul_output_0']\r\noutput: ['/Mul_output_0_quantized', '/Mul_output_0_scale', '/Mul_output_0_zero_point']\r\n\r\nname: /fc/Gemm_MatMul_quant_scales_mul\r\nop_type: Mul\r\ninput: ['/Mul_output_0_scale', 'fc.weight_scale']\r\noutput: ['/fc/Gemm_MatMul_quant_scales_mul:0']\r\n\r\nname: /fc/Gemm_MatMul_quant\r\nop_type: MatMulInteger\r\ninput: ['/Mul_output_0_quantized', 'fc.weight_quantized', '/Mul_output_0_zero_point', 'fc.weight_zero_point']\r\noutput: ['y_MatMul_output_quantized']\r\n\r\nname: y_MatMul_output_quantized_cast\r\nop_type: Cast\r\ninput: ['y_MatMul_output_quantized']\r\noutput: ['y_MatMul_output_quantized_cast_output']\r\n\r\n\r\nname: /fc/Gemm_MatMul_quant_output_scale_mul\r\nop_type: Mul\r\ninput: ['y_MatMul_output_quantized_cast_output', '/fc/Gemm_MatMul_quant_scales_mul:0']\r\noutput: ['y_MatMul']\r\n\r\nname: /fc/Gemm_Add\r\nop_type: Add\r\ninput: ['y_MatMul', 'fc.bias']\r\noutput: ['y']\n")),"\n",a.createElement(e.p,null,"결과를 보니 CustomModule에서 설정한 scale 값을 양자화 없이 입력 x에 곱하고난 후 Linear 레이어에 넣는 것을 확인할 수 있습니다.\r\n아마 ONNX에서 미리 지정해둔 형식의 레이어에 대해서만 Quantization을 진행하는 것 같아요.\r\nSmoothQuant를 적용하는 것이 어렵지 않겠네요."),"\n",a.createElement(e.h1,{id:"onnx-smoothquant"},"ONNX SmoothQuant"),"\n",a.createElement(e.p,null,"위와 같은 방식으로 ONNX에서 SmoothQuant를 적용하여 GPU를 이용한 추론을 진행할 수 있다는 것을 보았습니다.\r\n그렇다면 실제로 BERT 모델에 적용하여 테스트를 진행해볼게요."),"\n",a.createElement(e.pre,null,a.createElement(e.code,{className:"language-python"},"from datasets import load_dataset\r\nfrom transformers import BertTokenizer, BertForSequenceClassification\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\n\r\nimport onnxruntime as ort\r\n\r\nMODEL_NAME = 'yoshitomo-matsubara/bert-large-uncased-qnli'\r\nMAX_LENGTH = 256\r\nBATCH_SIZE = 8\r\n\r\nmodel = BertForSequenceClassification.from_pretrained(MODEL_NAME)\r\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\r\ndataset = load_dataset('glue', 'qnli', split=['test', 'validation'])\r\ndef tokenize(data):\r\n    return tokenizer(data['question'], data['sentence'], truncation=True, max_length=MAX_LENGTH, padding='max_length', return_tensors='pt')\r\n\r\ndef create_dataloader(dataset, batch_size):\r\n    input_ids = torch.tensor(dataset['input_ids'])\r\n    attention_masks = torch.tensor(dataset['attention_mask'])\r\n    token_type_ids = torch.tensor(dataset['token_type_ids'])\r\n    tensor_dataset = TensorDataset(input_ids, attention_masks, token_type_ids)\r\n    return DataLoader(tensor_dataset, batch_size = batch_size)\r\n\r\ntest_dataloader = create_dataloader(tokenize(dataset[0][:1000]), BATCH_SIZE)\r\nval_dataloader = create_dataloader(tokenize((dataset[1][:500])), BATCH_SIZE)\r\n\n")),"\n",a.createElement(e.p,null,"SmoothQuant가 작은 모델에 대해서 좋은 성능을 내지 못하지만,\r\n예전에 테스트를 진행했던 BERT-Large 모델의 QNLI 작업은 SmoothQuant를 적용하였을 때 기존의 Quantization 방식보다 더 높은 정확도를 보여주었기에 이를 활용하겠습니다."),"\n",a.createElement(e.pre,null,a.createElement(e.code,{className:"language-python"},"@torch.no_grad()\r\ndef predict(model, dataloader, device=torch.device('cuda')):\r\n    model.to(device)\r\n    model.eval()\r\n    \r\n    preds = []\r\n    for _, batch in enumerate(dataloader):\r\n        batch_inputs = tuple(t.to(device) for t in batch)\r\n        inputs = {\r\n            'input_ids': batch_inputs[0],\r\n            'attention_mask': batch_inputs[1],\r\n            'token_type_ids': batch_inputs[2]\r\n        }\r\n        \r\n        outputs = model(**inputs)\r\n        preds.append(outputs[0].argmax(dim=1))\r\n    \r\n    preds = torch.cat(preds)\r\n    return preds\r\n\r\npreds = predict(model, test_dataloader).cpu().numpy()\r\n\r\ndef to_onnx(model, onnx_file):\r\n    model.cpu() \r\n    dummy_input = tokenizer(\"This is a sample sentence\", return_tensors=\"pt\")\r\n    input_names = ['input_ids', 'attention_mask', 'token_type_ids']\r\n    output_names = ['logits']\r\n    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \r\n                'attention_mask': {0: 'batch_size', 1: 'sequence'},\r\n                'token_type_ids': {0: 'batch_size', 1: 'sequence'}, \r\n                'logits': {0: 'batch_size', 1: 'sequence'}}\r\n\r\n    torch.onnx.export(\r\n        model,\r\n        tuple(dummy_input.values()),\r\n        verbose=True,\r\n        f=onnx_file,\r\n        input_names=input_names,\r\n        output_names=output_names,\r\n        dynamic_axes=dynamic_axes,\r\n    )\r\n\r\nto_onnx(model, 'bert/bert-large.onnx')\n")),"\n",a.createElement(e.p,null,"ONNX 모델과 비교를 하기 위해 test 데이터를 통해 추론을 진행하였습니다.\r\n또 모델을 기본적인 Quantization 방법을 적용하여 테스트한 결과와 비교하기 위해"),"\n",a.createElement(e.pre,null,a.createElement(e.code,{className:"language-python"},"from functools import partial\r\n\r\ndef get_act_scales(model):\r\n    model.eval()\r\n    act_scales = {}\r\n\r\n    # get maximum channel values\r\n    def stat_tensor(name, tensor):\r\n        hidden_dim = tensor.shape[-1]\r\n        tensor = tensor.view(-1, hidden_dim).abs().detach()\r\n        comming_max = torch.max(tensor, dim=0)[0].float().cpu()\r\n        if name in act_scales:\r\n            act_scales[name] = torch.max(act_scales[name], comming_max)\r\n        else:\r\n            act_scales[name] = comming_max\r\n\r\n    def stat_input_hook(m, x, y, name):\r\n        if isinstance(x, tuple):\r\n            x = x[0]\r\n            stat_tensor(name, x)\r\n        \r\n    # register hook on every Linear layer\r\n    hooks = []\r\n    for name, m in model.named_modules():\r\n        if isinstance(m, nn.Linear):\r\n            hooks.append(\r\n                m.register_forward_hook(\r\n                    partial(stat_input_hook, name=name))\r\n            )\r\n    \r\n    predict(model, val_dataloader)\r\n    \r\n    # remove hooks\r\n    for h in hooks:\r\n        h.remove()\r\n    \r\n    return act_scales\r\n\r\nact_scales = get_act_scales(model, val_dataloader)\r\n\r\nclass SmoothLinear(nn.Module):\r\n    def __init__(self, linear, act_scale, alpha=0.5) -> None:\r\n        super().__init__()\r\n        weight = linear.weight.detach()\r\n        w_abs_max = weight.abs().max(dim=0)[0].clamp_(min=1e-5)\r\n        scale = act_scale.pow(alpha).div_(w_abs_max.pow(1 - alpha)).clamp_(min=1e-5)\r\n        linear.weight.data = weight * scale\r\n        \r\n        self.register_buffer(\"scale\", scale)\r\n        self.linear = linear\r\n        \r\n    def forward(self, x):\r\n        x = x / self.scale\r\n        return self.linear(x)\r\n\r\ndef smooth_quantize(model, act_scales):\r\n    model.cpu()\r\n    for name, act_scale in act_scales.items():\r\n        keys = name.split(\".\")\r\n        module = model\r\n        for key in keys[:-1]:\r\n            module = getattr(module, key)\r\n        \r\n        if 'query' in name or 'key' in name or 'value' in name or 'intermediate' in name:\r\n            setattr(module, keys[-1], SmoothLinear(getattr(module, keys[-1]), act_scale))\r\n            \r\nsmooth_quantize(model, act_scales)\r\n\r\nto_onnx(model, 'bert/bert-large-smooth.onnx')\n")))}var i=function(n){void 0===n&&(n={});const{wrapper:e}=Object.assign({},(0,r.ah)(),n.components);return e?a.createElement(e,n,a.createElement(o,n)):o(n)},l=t(4316),s=t(1840),u=t(7821),m=t(2654),d=t(4111),c=t(2726),_=t(4480),p=t(2818),h=t(9213),f=t(7213),y=t(9265),g=t(9601),x=t(3071),b=t(6097),E=t(6782),v=t(4891),M=t(3387),w=t(917);const z=n=>{let{data:{mdx:e},children:t}=n;const o=(0,_.sJ)((0,p.cp)(p.eE,!1)),i=(0,_.sJ)((0,p.cp)(p.rf,!1)),l=(0,_.Zl)((0,p.cp)(p.Cy,e.tableOfContents.items));return(0,a.useEffect)((()=>{l(e.tableOfContents.items)}),[e]),(0,w.tZ)(s.Z,null,(0,w.tZ)(N,null,(0,w.tZ)(d.Z,null)),(0,w.tZ)(k,null,(0,w.tZ)(Z,{className:"navigation",isNavOpened:i},(0,w.tZ)(q,{className:"hide_scroll"},(0,w.tZ)(u.Z,null))),(0,w.tZ)(j,{isNavOpened:i},(0,w.tZ)(O,{isWide:o},(0,w.tZ)(c.Z,{title:e.frontmatter.title,date:e.frontmatter.date}),(0,w.tZ)(r.Zo,{components:{p:f.Z,h1:y.H1,h2:y.H2,h3:y.H3,h4:y.H4,h5:y.H5,h6:y.H6,hr:g.Z,blockquote:x.Z,ul:E.Z,ol:b.Z,pre:v.Z,code:M.Z}},t))),(0,w.tZ)(L,null,(0,w.tZ)(Q,null,(0,w.tZ)(m.Z,null)))))},N=(0,l.Z)("div",{target:"e1ojob7j7"})({name:"11t2x7x",styles:"display:flex;height:var(--header-height);z-index:5;padding:0.6rem 2rem 0.6rem 0.6rem;position:fixed;width:100%;background:var(--background-color);border-bottom:1px solid var(--border-color)"}),k=(0,l.Z)("div",{target:"e1ojob7j6"})({name:"majwgz",styles:"position:relative;display:flex;min-height:calc(100vh - var(--header-height));overflow-x:hidden"}),Z=(0,l.Z)("aside",{target:"e1ojob7j5"})("margin-left:",(n=>n.isNavOpened?"0":"calc(-1 * var(--sidebar-width))"),";flex:0 0 var(--sidebar-width);font-size:0.875rem;overflow-x:hidden;overflow-y:auto;transition:margin 0.25s var(--ease-in-out-quad);@media (min-width: ",h.Z.IPAD_PRO,"px){margin-left:0;}"),q=(0,l.Z)("nav",{target:"e1ojob7j4"})({name:"l4vzaw",styles:"overflow-y:auto;height:100%;padding:var(--body-padding-top) 0 3rem 0;position:fixed;width:var(--sidebar-width);&:-webkit-scrollbar{display:none;}"}),O=(0,l.Z)("main",{target:"e1ojob7j3"})("padding:1rem;width:100%;@media (min-width: ",h.Z.IPAD_AIR,"px){width:",(n=>n.isWide?"90%":"65%"),";}"),j=(0,l.Z)("main",{target:"e1ojob7j2"})("width:calc(100% - 2 * var(--sidebar-width));padding-top:var(--body-padding-top);flex-grow:1;min-width:20rem;display:flex;justify-content:center;opacity:",(n=>n.isNavOpened?.3:1),";@media (min-width: ",h.Z.IPAD_PRO,"px){opacity:1;}"),L=(0,l.Z)("aside",{target:"e1ojob7j1"})("font-size:0.75rem;font-weight:bold;overflow-x:hidden;overflow-y:auto;padding-top:var(--body-padding-top);width:0;transition:width 0.25s var(--ease-in-out-quad);@media (min-width: ",h.Z.HD,"px){width:var(--sidebar-width);}"),Q=(0,l.Z)(q,{target:"e1ojob7j0"})({name:"b40oxt",styles:"padding:0 1rem 0 1rem"});function S(n){return a.createElement(z,n,a.createElement(i,n))}}}]);
//# sourceMappingURL=component---src-components-templates-main-template-tsx-content-file-path-contents-project-hummingbird-quantization-quantization-mdx-b05bf99ceb7cae2c635b.js.map