{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-pill-recognizer-text-reader-mdx","path":"/1561e289-dbb1-548f-9cfe-19a5157f6b3e","result":{"data":{"mdx":{"id":"1561e289-dbb1-548f-9cfe-19a5157f6b3e","body":"\r\n# 초기  개발\r\n\r\n## 유사도 측정\r\n\r\n후보군의 텍스트와 예측한 결과 텍스트의 유사도를 측정하기 위해 두 가지 기준을 사용할 거예요.  \r\n* Jaccard Similarity : 두 집합 간의 교집합과 합집합의 비율을 계산하여 유사도를 측정하는 방식  \r\n```python\r\nimport distance\r\nt1 = 'compliment'\r\nt2 = 'complement'\r\nprint(distance.jaccard(t1, t2)) \r\n# output : 0.1111\r\nt1 = 'naver'\r\nt2 = 'kakao'\r\nprint(distance.jaccard(t1, t2)) \r\n# output : 0.8571\r\nt1 = 'hanyang univ'\r\nt2 = 'SNU'\r\nprint(distance.jaccard(t1, t2)) \r\n# output : 1.0\r\n```\r\n* Levenshtein Distance : 두 텍스트를 서로 같게 만들기 위해 필요한 최소 편집(삽입, 삭제, 대체)\r\n```python\r\nimport nltk\r\nt1 = 'compliment'\r\nt2 = 'complement'\r\nprint(nltk.edit_distance(t1, t2)) \r\n# output : 1\r\nt1 = 'naver'\r\nt2 = 'kakao'\r\nprint(nltk.edit_distance(t1, t2)) \r\n# output : 4\r\nt1 = 'hanyang univ'\r\nt2 = 'SNU'\r\nprint(nltk.edit_distance(t1, t2))\r\n# output : 12\r\n```\r\n\r\nJaccard Similarity와 Levenshtein Distance는 두 텍스트가 유사할수록 낮은 값을 가지도록 되어 있어요.\r\n하지만 Levenshtein Distance는 하나의 텍스트가 길면 길수록 한없이 커진답니다.\r\n그래서 우리는 길이가 더 긴 텍스트 쪽으로 normalization을 해 줄 것입니다.\r\n\r\n```python\r\ndef normed_leven_dist(t1, t2):\r\n    max_size = max(len(t1), len(t2))\r\n    return nltk.edit_distance(t1, t2) / max_size\r\n\r\nprint(normed_leven_dist('compliment', 'complement')) \r\n# output : 0.1\r\nprint(normed_leven_dist('naver', 'kakao')) \r\n# output : 0.8\r\nprint(normed_leven_dist('hanyang univ', 'SNU')) \r\n# output : 1.0\r\n```\r\n\r\n이제 두 값을 모두 사용하기 때문에 두 값을 곱해서 확률로 만들어 줍시다.\r\n우리가 원하는 것은 유사도를 얻는 것이기에 유사성이 높을수록 1에 가까워야 맞겠죠?\r\n곱한 결과를 1에서 빼주는 형태로 구현하면 되겠네요.\r\n\r\n```python\r\ndef get_similarity(t1, t2):\r\n    max_len = max(len(t1), len(t2))\r\n    leven_dist = nltk.edit_distance(t1, t2) / max_len\r\n    jaccard_sim = distance.jaccard(t1, t2)\r\n    return 1 - leven_dist * jaccard_sim\r\n\r\nprint(get_similarity('compliment', 'complement')) \r\n# output : 0.9888\r\nprint(get_similarity('naver', 'kakao')) \r\n# output : 0.31428\r\nprint(get_similarity('hanyang univ', 'SNU')) \r\n# output : 0.0\r\n```\r\n\r\n사실 위 두 함수는 구현하는 것이 어렵지 않고 제가 의존성을 추가하는 것을 별로 좋아하지 않지만,\r\n많은 후보군과 비교를 위해서 최적화가 되어있는 라이브러리를 사용했어요.\r\n뭐 최적화가 잘 되어 있는지는 나중에 비교를 해보는 것으로 하죠.\r\n\r\n## 학습\r\n\r\n학습 데이터를 생성하기 위해서 [deep-text-recognition-benchmark](https://github.com/clovaai/deep-text-recognition-benchmark) 레포를 따를 것입니다.  \r\n일단 위 레포를 clone 합시다!\r\n\r\n먼저 우리는 아래와 같은 폴더 구조로 데이터를 가지고 있어요.\r\n```\r\n|-- data\r\n|   |-- train\r\n|       |-- gt.txt\r\n|       |-- boxes\r\n|           |-- 200900695_0.jpg\r\n|           |-- ...\r\n|   |-- val\r\n|       |-- gt.txt\r\n|       |-- boxes\r\n|           |-- 195900027_0.jpg\r\n|           |-- ...\r\n```\r\n```\r\n# gt.txt\r\n200900695_0.jpg\tAL\r\n...\r\n```\r\n\r\n자 이제 deep-text-recognition-benchmark/create_lmdb_dataset.py 코드를 통해 데이터셋을 생성할 거예요.\r\n그런데 create_lmdb_dataset.py 코드에서 약간 수정할 부분이 있어요.\r\n```python\r\n# create_lmdb_dataset.py\r\ndef createDataset(inputPath, gtFile, outputPath, checkValid=True):\r\n    \"\"\"\r\n    Create LMDB dataset for training and evaluation.\r\n    ARGS:\r\n        inputPath  : input folder path where starts imagePath\r\n        outputPath : LMDB output path\r\n        gtFile     : list of image path and label\r\n        checkValid : if true, check the validity of every image\r\n    \"\"\"\r\n    os.makedirs(outputPath, exist_ok=True)\r\n    env = lmdb.open(outputPath, map_size=1024000000) # <- 여기서 map size를 데이터 크기에 따라 조절 하세요!\r\n    cache = {}\r\n    cnt = 1\r\n```\r\n\r\n이제 train, val 데이터로 학습에 사용할 lmdb 데이터를 생성합시다.\r\n```sh\r\n$ python deep-text-recognition-benchmark/create_lmdb_dataset.py --inputPath data/train/boxes --gtFile data/train/gt.txt --outputPath data/train/lmdb\r\n$ python deep-text-recognition-benchmark/create_lmdb_dataset.py --inputPath data/val/boxes --gtFile data/val/gt.txt --outputPath data/val/lmdb\r\n```\r\n\r\n자 lmdb 데이터도 생성이 되었겠다, 학습을 시작합니다.\r\n여러가지 파라미터들이 존재하는데 모델에 관련한 몇몇 파라미터와 예측에 사용할 문자들을 한정해주어 학습을 진행할 거예요.\r\n```sh\r\npython deep-text-recognition-benchmark/train.py --train_data data/train/lmdb --valid_data data/val/lmdb --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --select_data '/' --batch_ratio 1 --workers 0 --batch_size 128 --character '&+-./0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghiklmnoprstuvwxyz'\r\n```\r\n\r\n폴더가 하나 새로 생겼죠?\r\n로그도 있고 best_accuracy.pth, best_norm_ED.pth가 있어요.\r\nnorm_ED는 normalized edit distance라고 하네요.\r\n위에서 사용했던 'Levenshtein Distance'의 normalized 버전이 아닐까 생각이 듭니다.\r\n\r\n## 테스트\r\n\r\n이제 학습된 모델을 불러와서 추론하는 것을 살펴봅시다.\r\n```python\r\nimport easyocr\r\nimport matplotlib.pyplot as plt\r\n\r\ndef show_img(img_path):\r\n    plt.imshow(plt.imread(img_path))\r\n    plt.axis('off')  # Optional: to hide the axis\r\n    plt.show()\r\n\r\nimg_path = '196000001_0.jpg'\r\nshow_img(img_path)\r\n\r\nreader = easyocr.Reader(['en'], user_network_directory='../data')\r\nprint(reader.readtext(img_path))\r\n```\r\n\r\n'196000001_0.jpg' 이미지에 대해 텍스트 인식을 해보았어요.\r\n\r\n![196000001_0](assets/text_reader/01.png)\r\n```\r\n[([[97, 61], [217, 61], [217, 97], [97, 97]], 'PSCK', 0.9858795404434204), ([[118, 90], [194, 90], [194, 122], [118, 122]], '500', 0.9943707547542567)]\r\n```\r\n\r\n위와 같이 리스트의 형태로 위치, 텍스트, 그리고 score가 함께 나오네요.\r\n우리에게 필요한 것은 텍스트와 score!\r\nscore가 0.1만 되어도 정확하지 않지만 비슷한 모양의 텍스트가 나오더라구요.\r\n그래서 일정 threshold 이상의 score가 나오면 결과에 포함시키는 형태로 모듈을 구현해 보았습니다.\r\n\r\n```python\r\n# text_reader.py\r\nimport easyocr\r\n\r\nclass TextReader():\r\n    def __init__(self, model_path, threshold=0.1) -> None:\r\n        self.reader = easyocr.Reader(['en'], user_network_directory=model_path)\r\n        self.threshold = threshold\r\n        \r\n    def read(self, img):\r\n        text = ''\r\n        results = self.reader.readtext(img)\r\n        for result in results:\r\n            pred = result[1]\r\n            conf = result[2]\r\n            if conf > self.threshold:\r\n                text += pred\r\n        return text\r\n```\r\n\r\n이제 위 모듈을 이용하여 정확도를 평가할 생각입니다.\r\n우리는 인식할 문자를 어느정도 한정하여 학습을 진행했지만, 사실 프린팅된 텍스트가 아닌 문자를 정확하게 인식하는 것은 쉽지 않습니다.\r\n그래서 예측한 텍스트와 정확히 일치하는 알약을 찾는 방법이 아닌, 다른 특징들로 좁혀진 후보군 중 가장 유사한 텍스트를 가진 알약을 선정하는 방법으로 접근할 거예요.\r\n텍스트 인식 모듈 단일 테스트기 때문에 색, 모양은 테스트 하려는 알약의 데이터를 그대로 사용하고 텍스트만 예측한 값을 사용하는 식으로 진행할게요.\r\n\r\n```python\r\ndef predict_by_feats(db, shape, color, text, k=5):\r\n    candidates = db[db['의약품제형']==shape]\r\n    candidates = candidates[(candidates['색상앞']==color) | (candidates['색상뒤']==color)]\r\n    \r\n    cand_probs = []\r\n    for _, row in candidates.iterrows():\r\n        text_front = preprocess_text(row['표시앞'])\r\n        text_back = preprocess_text(row['표시뒤'])\r\n        cand_probs.append(max(get_similarity(text_front, text), get_similarity(text_back, text)))\r\n    \r\n    cand_probs = np.array(cand_probs)\r\n    best_idx = cand_probs.argmax()\r\n    return candidates.iloc[best_idx]['품목일련번호']\r\n```\r\n\r\n이렇게 만들어서 테스트를 해보았더니, 결과가 처참하더라구요.\r\n나름대로 원인을 좀 분석해보니 아래와 같은 문제점이 보였습니다/\r\n\r\n![same feats](assets/text_reader/00.png)\r\n\r\n위 두 알약은 모양도, 색도, 텍스트도 동일해요.\r\n사람의 눈으로 자세히 보면 비교할 수 있지만, 우리가 가진 특징 만으로 컴퓨터가 판단하긴 어려워 보여요.\r\n그래서 하나의 결과만 던져주는 방법이 아닌, 가장 근접한 N개의 결과를 던져주고 유저가 자신의 알약과 가장 비슷한 약을 고르도록 가이드를 제시할 거예요.\r\n그렇기 때문에 우리는 텍스트 인식 모듈의 정확도를 측정할 때, N개의 결과 중에 답이 존재한다면 옳은 예측이라 판단합시다.\r\n```python\r\ndef predict_by_feats(db, shape, color, text, k=5):\r\n    candidates = db[db['의약품제형']==shape]\r\n    candidates = candidates[(candidates['색상앞']==color) | (candidates['색상뒤']==color)]\r\n    \r\n    cand_probs = []\r\n    for _, row in candidates.iterrows():\r\n        text_front = preprocess_text(row['표시앞'])\r\n        text_back = preprocess_text(row['표시뒤'])\r\n        cand_probs.append(max(get_similarity(text_front, text), get_similarity(text_back, text)))\r\n    \r\n    cand_probs = np.array(cand_probs)\r\n    top_k = cand_probs.argsort()[-k:]\r\n    results = [candidates.iloc[i]['품목일련번호'] for i in top_k]\r\n    return results\r\n```\r\n\r\n```\r\nvalid predictions: 317, num data: 751, accuracy: 42.2103861517976\r\n```\r\n\r\nTop N개를 선정하는 방식도 정확도가 그리 좋지만은 않네요.\r\n아마 텍스트간 유사성을 구하는 부분에서 문제가 있는 것 같아요.\r\n\r\n```python\r\nprint(get_similarity('atlck', 'at|ck'))\r\nprint(get_similarity('atck', 'at|ck'))\r\n```\r\n\r\n```\r\n0.93\r\n0.96\r\n```\r\n\r\n위 예시를 보면 첫 번째 경우는 '|'과 'l'을 단순히 잘못 예측한 경우에요.\r\n두 문자는 자세히 보지 않으면 실제 세상에서도 다르지 않아 보일 정도로 유사합니다.\r\n그래서 두 번째 경우처럼 아예 '|'를 인식하지 못하는 경우보다 점수가 더 높아야 한다고 생각해요.\r\n하지만 우리가 구현한 방식은 두 번째 경우를 더 높은 확률로 인식하였네요.\r\n어쩌면 모듈을 개선하는 방법은 더 정밀한 모델을 만드는 방법 보다 뛰어난 유사성 알고리즘을 찾는 것이,\r\n큰 차이로 성능을 높일 수 있을 것 같아요.","tableOfContents":{"items":[{"url":"#초기--개발","title":"초기  개발","items":[{"url":"#유사도-측정","title":"유사도 측정"},{"url":"#학습","title":"학습"},{"url":"#테스트","title":"테스트"}]}]},"frontmatter":{"description":"Recognize text of pill","title":"Text Recognition"}},"file":{"modifiedTime":"2023-12-15T12:04:17.492Z"}},"pageContext":{"id":"1561e289-dbb1-548f-9cfe-19a5157f6b3e","relativePath":"pill_recognizer/text_reader.mdx","frontmatter":{"title":"Text Recognition","description":"Recognize text of pill"}}},"staticQueryHashes":["123912876","2317542362"],"slicesMap":{}}