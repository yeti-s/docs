{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-true-title-detector-mdx","path":"/e6f11b71-1b53-5109-bb9e-38278de5d482","result":{"data":{"mdx":{"id":"e6f11b71-1b53-5109-bb9e-38278de5d482","body":"\r\nimport Comment from '@contents/components/Comment';\r\nimport ColorText from '@contents/components/ColorText';\r\n\r\n기사의 제목과 내용을 보고 낚시성 기사 여부를 판단하는 모듈을 개발합니다.\r\n\r\n# Base 모델 선정\r\n\r\n높은 한국어 인식율을 위해 한국어 문장으로 pre-trained 모델을 찾아보았어요. \r\n\r\n* [skt/kobert-base-v1](https://github.com/SKTBrain/KoBERT)  \r\nSKT에서 제공하는 KoBERT는 의존성을 추가로 설치해야해서 다른 모델을 더 찾아보고 결정하기로 할게요.\r\n* [monologg/kobert](https://huggingface.co/monologg/kobert)  \r\nSKT KoBERT 다음으로 다운로드 수가 많은 모델이 huggingface에 있더라구요.\r\n이를 사용하기로 합시다.\r\n\r\n# 데이터 수집\r\n\r\n우리는 기사의 제목과 내용을 보고 낚시성 여부를 판단하려고 합니다.\r\n이에 뉴스 기사에 대한 내용과 제목을 크롤링하여 데이터로 만들 필요가 있어요.\r\n하지만 모든 제목과 내용을 하나하나 비교하여 낚시성 여부를 판단 분류하는 것은 현실적으로 어려움이 있어요.\r\n그래서 우리는 기사의 내용과 또 다른 기사의 제목을 사용하여 낚시성 기사를 생산해낼 생각입니다.\r\n\r\n![](assets/detector/02.png)\r\n\r\n# Longformer 적용\r\n\r\n학습을 진행하기 위해 뉴스 기사 내용을 토큰화하다 보니 최대 토큰 수를 넘어가는 일이 자주 발생하더라구요.\r\n내용이 긴 뉴스에 대해 어느정도 내용의 손실이 발생하는 것입니다.\r\n이를 해결하기 위해 BERT의 설정과 embedding layer를 어느정도 custom하여 최대 토큰 수를 늘리려고 하였으나 이는 메모리 사용량이 증가하다 보니 효율적이지 못한 것 같았아요.\r\n그래서 여러가지 방법을 찾아보았습니다.\r\n\r\n![](assets/detector/00.png)\r\n\r\n그 중 `Longformer`라는 모델이 긴 문서에 대한 처리를 가능하게 한다는 논문을 보았어요.\r\n위 그림에서 (b), (c), (D) 처럼 3 가지 테크닉을 사용하여 기존의 fully self-attention을 조금 변형하여 적은 메모리를 사용하고도 효과적인 성능을 내는 원리입니다.\r\n최대 토큰 수가 4096이라 하여 기사 내용을 토큰화 하기엔 충분한 것 같았어요.\r\n하지만 한국어로 pre-trained된 모델이 없어 fine-tuning을 해도 기존보다 더 좋은 성능을 낼 수 있을지 의문이 들었어요.\r\n그래서 이미 한국어로 pre-trained된 KoBERT 모델에 Longformer의 self-attention을 적용하는 방향을 생각하였습니다.\r\n\r\n<Comment>\r\nBERT 모델의 최대 토큰 수는 512 입니다.\r\n</Comment>\r\n\r\n먼저 huggingface에 이미 구현되어 있는 LongformerSelfAttention를 이용하여 BERT에 적용하려고 해요.\r\nBertSelfAttention와 LongformerSelfAttention의 input이 차이가 있기 때문에 LongformerSelfAttentionForBERT 선언해 적절한 input으로 맞춰줍시다.\r\n\r\n```python\r\nclass LongBertSelfAttention(nn.Module):\r\n    def __init__(self, config, layer_id):\r\n        super().__init__()\r\n        self.long_self_attn = LongformerSelfAttention(config, layer_id=layer_id)\r\n        \r\n    def forward(\r\n        self,\r\n        hidden_states: torch.Tensor,\r\n        attention_mask: Optional[torch.FloatTensor] = None,\r\n        head_mask: Optional[torch.FloatTensor] = None,\r\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\r\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\r\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\r\n        output_attentions: Optional[bool] = False,\r\n    ):\r\n\r\n        # bs x seq_len x seq_len -> bs x seq_len 으로 변경\r\n        batch_size, seq_len, _ = hidden_states.size()\r\n        # attention_mask = attention_mask.squeeze()\r\n        attention_mask = attention_mask.view(batch_size, seq_len)\r\n\r\n        is_index_masked = attention_mask < 0\r\n        is_index_global_attn = attention_mask > 0\r\n        is_global_attn = is_index_global_attn.flatten().any().item()\r\n\r\n        outputs = self.long_self_attn(\r\n            hidden_states,\r\n            attention_mask=attention_mask,\r\n            layer_head_mask=None,\r\n            is_index_masked=is_index_masked,\r\n            is_index_global_attn=is_index_global_attn,\r\n            is_global_attn=is_global_attn,\r\n            output_attentions=output_attentions,\r\n        )\r\n\r\n        return outputs\r\n```\r\n\r\nis_index_masked, is_index_global_attn, is_global_attn 는 나중에 정리\r\n\r\n또 우리가 수정한 모델에 대해 huggingface에 업로드하고 나중에 불러올 수 있도록 하기 위해 모델에 대한 class를 정의해줍시다.\r\n```python\r\nclass LongBertForSequenceClassification(BertForSequenceClassification):\r\n    def __init__(self, config):\r\n        super().__init__(config)\r\n        for i, layer in enumerate(self.bert.encoder.layer):\r\n            layer.attention.self = LongBertSelfAttention(config, layer_id=i)\r\n```\r\n\r\n자 이제 모델에 대해서 설정값을 수정하고 self attention 레이어를 변경해줄 차례에요.\r\n먼저 BERT의 설정에서 최대 토큰 수를 변경하고 attention window의 크기를 설정해줍시다.\r\n```python\r\nmax_token_size = 4096\r\nconfig = model.config\r\nconfig.max_position_embeddings = max_token_size\r\nconfig.attention_window = [attention_window] * config.num_hidden_layers\r\n```\r\n\r\n이제 position embedding의 크기를 늘려줘야 해요.\r\n기존에는 512가 최대이기 떄문에 position_embeddings의 weight이 (512, 768)의 형태로 되어 있을거예요.\r\n우리는 이를 (max_token_size, 768)의 형태로 늘려주는 작업을 진행합니다.\r\n그리고 position_ids 역시 0~511에서 0~(max_token_size - 1)로 범위를 늘려줘야 합니다.\r\n```python\r\ncurrent_max_token_size = model.bert.embeddings.position_embeddings.weight.size(0)\r\nnum_repeats = math.ceil(max_token_size / current_max_token_size)\r\nembed_weight = model.bert.embeddings.position_embeddings.weight.data\r\nembed_weight = embed_weight.repeat(num_repeats, 1)[:max_token_size,:]\r\nmodel.bert.embeddings.position_embeddings.weight.data = embed_weight\r\nmodel.bert.embeddings.position_ids.data = torch.arange(0,max_token_size).view(1, -1)\r\n```\r\n\r\n<Comment>\r\n이때 조심해야 해요. RoBERTa와 같이 positional_ids 에서 0, 1을 특별한 용도로 사용하기 위한 모델도 있기 때문입니다.\r\n잘 조사해보고 변경해야겠죠?\r\n</Comment>\r\n\r\n이제 self attention 레이어를 변경해줍시다.\r\nLongformer의 self attention은 query, key, value뿐 아니라 query_global, key_global, value_global이 있습니다.\r\n각각 기존 BERT의 query, key, value와 동일한 값을 가지도록 만들어 줍시다.\r\n```python\r\nfor i, layer in enumerate(model.bert.encoder.layer):\r\n    long_bert_self_attn = LongBertSelfAttention(config, layer_id=i)\r\n    long_bert_self_attn.long_self_attn.query = layer.attention.self.query\r\n    long_bert_self_attn.long_self_attn.key = layer.attention.self.key\r\n    long_bert_self_attn.long_self_attn.value = layer.attention.self.value\r\n    long_bert_self_attn.long_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\r\n    long_bert_self_attn.long_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\r\n    long_bert_self_attn.long_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\r\n    layer.attention.self = long_bert_self_attn\r\n```\r\n\r\n단순히 기존 BERT모델의 max token과 embedding만 늘렸을 때와\r\nLongformer의 self-attention 방식을 적용하였을 때 메모리 사용량 차이를 비교해 볼게요.\r\n동일한 길이의 토큰을 이용하고, LongBERT의 경우 다양한 attention window을 설정하여 테스트를 진행하겠습니다.\r\n\r\n![](assets/detector/01.png)\r\n\r\n기존의 BERT 모델에 비해 LongBERT의 경우 학습, 추론시 모두 더 적은 메모리 용량을 사용하는 것을 볼 수 있습니다.\r\n또한 최대 토큰수 증가에 따른 메모리 사용량이 기존 BERT에 비해 LongBERT가 더 적은 폭으로 증가하는 것을 확인할 수 있어요.\r\n즉, 문서의 길이가 길면 길수록 LongBERT를 이용하여 더 효율적으로 학습과 추론을 수행할 수 있겠어요.\r\n","tableOfContents":{"items":[{"url":"#base-모델-선정","title":"Base 모델 선정"},{"url":"#데이터-수집","title":"데이터 수집"},{"url":"#longformer-적용","title":"Longformer 적용"}]},"frontmatter":{"description":"module to dectect clickbait articles","title":"Clickbait Detector"}},"file":{"modifiedTime":"2023-12-15T15:14:43.642Z"}},"pageContext":{"id":"e6f11b71-1b53-5109-bb9e-38278de5d482","relativePath":"true_title/detector.mdx","frontmatter":{"title":"Clickbait Detector","description":"module to dectect clickbait articles"}}},"staticQueryHashes":["123912876","2317542362"],"slicesMap":{}}