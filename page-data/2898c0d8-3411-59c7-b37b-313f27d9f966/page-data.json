{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-nlp-rnn-mdx","path":"/2898c0d8-3411-59c7-b37b-313f27d9f966","result":{"data":{"mdx":{"id":"2898c0d8-3411-59c7-b37b-313f27d9f966","body":"\r\nimport Comment from '@contents/components/Comment';\r\nimport ColorText from '@contents/components/ColorText';\r\n\r\n언어 모델은 다음 단어가 무엇이 오는지 예측하는 작업이에요.\r\n구체적으로 말하자면 연속적인 단어 $x^{(1)}, x^{(2)}, \\cdots, x^{(t)}$가 주어질 때, 다음 단어 $x^{(t+1)}$의 확률을 예측하는 것이죠.\r\n$$\r\nP(x^{(t+1)}|x^{(t)}, \\cdots, x^{(1)})\r\n$$\r\n\r\n이러한 시스템을 `language model`이라고 해요.\r\n조금 더 생각해보면 언어 모델은 [EOS]이 나올 때 까지 문장의 모든 부분을 확률로 할당하는 시스템인 셈이죠.\r\n\r\n$$\r\nP(x^{(1)}, \\cdots, x^{(T)}) = P(x^{(1)}) \\times P(x^{(2)}|x^{(1)}) \\times \\cdots \\times P(x^{(T)}|x^{(T-1)}, \\cdots, x^{(1)})\r\n$$\r\n\r\n# N-Gram Language Model\r\n\r\n`N-Gram Language Models`은 딥 러닝 이전 고전적인 언어 모델을 구현하는 방법입니다.\r\nn-gram은 연속된 단어 덩어리에요.\r\n아래 예시를 보면 이해가 쉬울 것입니다.\r\n\r\n```\r\nthe cat was under the bed\r\n\r\nunigrams: \"the\", \"cat\", \"was\", \"under\", \"the\", \"bed\"\r\nbigrams: \"the cat\", \"cat was\", \"was under\", \"under the\", \"the bed\"\r\ntrigrams: \"the cat was\", \"cat was under\", \"was under the\", \"under the bed\"\r\n```\r\n\r\n`Markov assumption`은 이러한 덩어리를 이용하여 다음 단어가 무엇이 나올지 예측하는 방식입니다.\r\n하나의 큰 corpus에서 나올 확률을 세는 방식이죠.\r\n아래 4-gram 언어 모델의 예시를 보겠습니다.\r\n\r\n$$\r\nP(w | students \\; opened \\; their) = \\frac{count(students \\; opened \\; their \\; w)}{count(students \\; opened \\; their)}\r\n$$\r\n\r\n여기서 \"student opened thier\"이 총 1000번 등장한다고 합시다.  \r\n\"student opened their books\"가 400번 등장하면 $P(books | student \\; opened \\; their) = 0.4$.  \r\n\"student opened their exams\"가 100번 등장하면 $P(exams | student \\; opened \\; their) = 0.1$.  \r\n이런 식으로 계산이 되는 것이죠.\r\n\r\n하지만 여기에는 몇 가지 문제가 있습니다.\r\n\r\n<ColorText color='var(--primary)'>Sparsity problem</ColorText>  \r\n- \"students opened their w\" 라는 문장이 전혀 등장하지 않는다면 w에 대한 확률은 0이 됩니다.  \r\n이럴 때에는 모든 $w \\in V$에 대해 카운트를 하여 적용하는 방법이 있을거에요. `smoothing`\r\n- \"students opened their\" 라는 문장이 전혀 등장하지 않는다면 \"opened theire\"로 대체하는 방법이 있을 거에요.\r\n여기서 n이 클수록 이 문제가 자주 발생할 확률이 높겠죠.\r\n\r\n<ColorText color='var(--primary)'>Storage problem</ColorText>  \r\n- 사용할 모든 n-gram의 수를 저장해야 하는 문제가 있죠.\r\n이는 n이 커지고, corpus가 클수록 모델 사이즈가 커진다는 것이죠.\r\n\r\nn-gram을 이용하여 아래와 같은 방식으로 문장을 생성할 수 있어요.\r\n\r\n![](assets/rnn/00.png)\r\n![](assets/rnn/01.png)\r\n\r\n놀랍게도 문법적으로는 틀리지 않지만, 문맥에 맞지 않는 내용이 만들어지죠.\r\n자연스럽게 문맥을 생성하려면 n을 키워야하고, 이는 모델 사이즈가 커지고 sparsity problem이 커지는 문제가 발생하죠.\r\n\r\n# RNN\r\n\r\n위 문제를 해결하기 위해서 언어 모델에 인공 신경망을 사용하는 방법이 등장했어요.\r\n\r\n![](assets/rnn/02.png)\r\n\r\nSparsity problem을 해결하고 모든 n-gram을 저장할 필요가 없어졌어요.\r\n하지만 window를 키우기 위해서는 $W$도 커져야 하기에 window를 충분히 크게 만들 수 없습니다.\r\n그래서 어떤 길이의 입력이든 처리할 수 있는 방법이 필요합니다.\r\n\r\n![](assets/rnn/03.png)\r\n\r\n`RNN`의 기본 아이디어는 입력에 대해 동일한 $W$를 사용하는 것이에요.\r\nRNN은 아래와 같은 특징을 가집니다.\r\n\r\n* 입력에 길이에 상관없이 처리할 수 있어요.\r\n* 이론적으로 $t$ 시점에서 정보는 이전 시점에서 모든 정보를 가지고 있어요.\r\n* 긴 입력이 들어와도 모델 사이즈는 커지지 않아요.\r\n* <ColorText color='var(--primary)'>연산 속도가 느립니다.</ColorText>\r\n* <ColorText color='var(--primary)'>실제로 오래 전 시점의 정보에 접근하긴 어렵습니다.</ColorText>\r\n\r\n## Training RNN\r\n\r\n![](assets/rnn/04.png)\r\n\r\n$$\r\n\\frac{\\partial J}{\\partial W} = \\frac{1}{T} \\sum_{t=1}^{T} J^{(t)} (\\theta) \\tag{1}\r\n$$\r\n\r\nRNN을 학습하기 위해서 모든 결과에 대해 loss를 계산하고 이를 평균내 전체 loss를 구합니다.\r\n하지만 실제로 우리가 다루는 입력은 문장이거나 어쩌면 문서가 될 수 있어요.\r\n이런 방식으로 전체 corpus의 loss를 구하는 작업은 큰 연산을 필요로 하게 되죠.\r\n이를 SGD와 같은 방식으로 어느정도 해결을 할 수 있겠죠.\r\n\r\n![](assets/rnn/10.png)\r\n혹은 전체 loss에 대해 계산하지 않고, 일부 길이마다 잘라 계산하는 형식으로 연산량을 줄일 수 있을거에요.\r\n\r\n![](assets/rnn/05.png)\r\n\r\n각 step마다 loss를 구하는 식은 아래와 같습니다.\r\n\r\n$$\r\nh^{(t)} = \\sigma (W_x x^{(t)} + W_h h^{(t-1)}) \\tag{2}\r\n$$\r\n\r\n$$\r\n\\frac{\\partial J^{(4)}}{\\partial W} = \r\n\\frac{\\partial J^{(4)}}{\\partial \\hat{y}^{(t)}} \\frac{\\partial \\hat{y}^{(t)}}{\\partial h^{(t)}}\r\n\\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} \\cdots\r\n\\frac{\\partial h^{(1)}}{\\partial W} \\tag{3}\r\n$$\r\n\r\n\r\n\r\n## Generate & Evaluate\r\n\r\nRNN을 이용하여 텍스트를 생성하는 과정은 다음과 같아요.\r\n\r\n![](assets/rnn/06.png)\r\n\r\nhidden state를 통해 예측한 단어를 다음 시점의 입력으로 사용하는 것이죠.\r\n이렇게 [EOS]라는 단어가 나올 때 까지 반복하여 문장을 생성하는 것입니다.\r\n이때 생성된 확률 분포에서 무작위로 단어를 선택하여 예측하는 `repeated sampling`을 사용합니다.\r\n이는 확률이 높은 단어가 자주 선택되겠지만, 낮은 확률을 가진 출력도 선택될 수 있어요.\r\n\r\n$$\r\nperplexity = \\prod_{t=1}^{T} (\\frac{1}{P_{LM} (x^(t+1) | x^(t), \\cdots, x^(1))}) ^ {1/T}\r\n$$\r\n\r\n이제 우리는 언어 모델의 평가 지표로 `perplexity`를 사용할거예요.\r\n언어 모델에서 이전 단어들이 주어졌을 때 다음 단어에 대한 확률의 역수를 모두 곱하여 $\\frac{1}{T}$승으로 normalization 한 것이죠.\r\n이는 cross-entrophy를 이용한 loss $J(\\theta)$의 exponential을 한 것과 같습니다.\r\n\r\n$$\r\n\\prod_{t=1}^{T} (\\frac{1}{P_{LM} (x^(t+1) | x^(t), \\cdots, x^(1))}) ^ {1/T} = \r\n\\prod_{t=1}^{T}(\\frac{1}{\\hat{y}_{x_{t+1}}^{(t)}})^{1/T} =\r\n\\exp (\\frac{1}{T} \\sum_{t=1}^{T} - \\log \\hat{y}_{x_{t+1}}^{(t)}) = \\exp (J(\\theta))\r\n$$\r\n\r\n우리는 loss가 낮아지도록 모델을 학습시켜요.\r\n즉 perplexity는 점수가 낮을수록 잘 학습된 모델이라고 할 수 있겠네요.\r\n\r\n뭐 문장 생성작업 외에도 entity recognition, text classification 등의 작업을 수행할 수 있어요.\r\n\r\n![](assets/rnn/07.png)\r\n![](assets/rnn/08.png)\r\n\r\n# LSTM, RNN Variants\r\n\r\n![](assets/rnn/09.png)\r\n\r\n우리는 학습을 위해 식 (3)과 같이 loss에 대한 기울기를 계산하였습니다.\r\n\r\n$$\r\n\\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} = \r\n\\frac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times\r\n\\frac{\\partial h^{(3)}}{\\partial h^{(2)}} \\times\r\n\\cdots \\times \\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} \\tag{4}\r\n$$\r\n\r\n$$\r\n\\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} = W_h h^{(t-1)} \\tag{5}\r\n$$\r\n\r\n\r\n식 (3)을 풀어보면 식 (4)의 형태가 등장하는데 식 (5)를 활용하면 $W_h$가 여러번 곱해지는 형태를 가지는 것을 볼 수 있어요.\r\n만약 step $t$가 크면 클수록 $W_h$가 더 많이 곱해지는 모습이 보이겠죠.\r\n그런데 만약 $\\left| W_h \\right| > 1$ 이라면 step이 길어질수록 loss에 대한 $W_h$의 기울기는 점점 더 커질거에요.\r\n이 상태로 gradient descent를 한다면 minimum과 멀어지는 쪽으로 학습이 진행될 것입니다.\r\n이를 `exploding gradient`라고 합니다.\r\n이를 해결하기 위해서 기울기가 일정 크기 이상 커지면 적당한 값으로 clipping하는 방식으로 해결할 수 있을거예요.\r\n\r\n반대로 $\\left| W_h \\right| < 1$ 이라면 step이 길어질수록 loss에 대한 $W_h$의 기울기는 점점 더 작아질거에요.\r\n이를 `vanishing gradient`라고 합니다.\r\nloss를 계산할 때 너무 먼 step과의 기울기를 구하는 것은 문제가 발생하게 됩니다.\r\n\r\n## LSTM\r\n\r\nvanishing gradient 문제를 해결하기 위해 등장한 모델 구조가 `LSTM`입니다.\r\n","tableOfContents":{"items":[{"url":"#n-gram-language-model","title":"N-Gram Language Model"},{"url":"#rnn","title":"RNN","items":[{"url":"#training-rnn","title":"Training RNN"},{"url":"#generate--evaluate","title":"Generate & Evaluate"}]},{"url":"#lstm-rnn-variants","title":"LSTM, RNN Variants","items":[{"url":"#lstm","title":"LSTM"}]}]},"frontmatter":{"description":"RNNs and Language Models","title":"RNN"}},"file":{"modifiedTime":"2023-12-13T06:35:50.426Z"}},"pageContext":{"id":"2898c0d8-3411-59c7-b37b-313f27d9f966","relativePath":"nlp/rnn.mdx","frontmatter":{"title":"RNN","description":"RNNs and Language Models","order":3}}},"staticQueryHashes":["2317542362","3731189812"],"slicesMap":{}}