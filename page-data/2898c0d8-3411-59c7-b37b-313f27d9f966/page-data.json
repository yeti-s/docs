{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-nlp-rnn-mdx","path":"/2898c0d8-3411-59c7-b37b-313f27d9f966","result":{"data":{"mdx":{"id":"2898c0d8-3411-59c7-b37b-313f27d9f966","body":"\r\nimport Comment from '@contents/components/Comment';\r\nimport ColorText from '@contents/components/ColorText';\r\n\r\n언어 모델은 다음 단어가 무엇이 오는지 예측하는 작업이에요.\r\n구체적으로 말하자면 연속적인 단어 $x^{(1)}, x^{(2)}, \\cdots, x^{(t)}$가 주어질 때, 다음 단어 $x^{(t+1)}$의 확률을 예측하는 것이죠.\r\n$$\r\nP(x^{(t+1)}|x^{(t)}, \\cdots, x^{(1)})\r\n$$\r\n\r\n이러한 시스템을 `language model`이라고 해요.\r\n조금 더 생각해보면 언어 모델은 [EOS]이 나올 때 까지 문장의 모든 부분을 확률로 할당하는 시스템인 셈이죠.\r\n\r\n$$\r\nP(x^{(1)}, \\cdots, x^{(T)}) = P(x^{(1)}) \\times P(x^{(2)}|x^{(1)}) \\times \\cdots \\times P(x^{(T)}|x^{(T-1)}, \\cdots, x^{(1)})\r\n$$\r\n\r\n# N-Gram Language Model\r\n\r\n`N-Gram Language Models`은 딥 러닝 이전 고전적인 언어 모델을 구현하는 방법입니다.\r\nn-gram은 연속된 단어 덩어리에요.\r\n아래 예시를 보면 이해가 쉬울 것입니다.\r\n\r\n```\r\nthe cat was under the bed\r\n\r\nunigrams: \"the\", \"cat\", \"was\", \"under\", \"the\", \"bed\"\r\nbigrams: \"the cat\", \"cat was\", \"was under\", \"under the\", \"the bed\"\r\ntrigrams: \"the cat was\", \"cat was under\", \"was under the\", \"under the bed\"\r\n```\r\n\r\n`Markov assumption`은 이러한 덩어리를 이용하여 다음 단어가 무엇이 나올지 예측하는 방식입니다.\r\n하나의 큰 corpus에서 나올 확률을 세는 방식이죠.\r\n아래 4-gram 언어 모델의 예시를 보겠습니다.\r\n\r\n$$\r\nP(w | students \\; opened \\; their) = \\frac{count(students \\; opened \\; their \\; w)}{count(students \\; opened \\; their)}\r\n$$\r\n\r\n여기서 \"student opened thier\"이 총 1000번 등장한다고 합시다.  \r\n\"student opened their books\"가 400번 등장하면 $P(books | student \\; opened \\; their) = 0.4$.  \r\n\"student opened their exams\"가 100번 등장하면 $P(exams | student \\; opened \\; their) = 0.1$.  \r\n이런 식으로 계산이 되는 것이죠.\r\n\r\n하지만 여기에는 몇 가지 문제가 있습니다.\r\n\r\n<ColorText color='var(--primary)'>Sparsity problem</ColorText>  \r\n- \"students opened their w\" 라는 문장이 전혀 등장하지 않는다면 w에 대한 확률은 0이 됩니다.  \r\n이럴 때에는 모든 $w \\in V$에 대해 카운트를 하여 적용하는 방법이 있을거에요. `smoothing`\r\n- \"students opened their\" 라는 문장이 전혀 등장하지 않는다면 \"opened theire\"로 대체하는 방법이 있을 거에요.\r\n여기서 n이 클수록 이 문제가 자주 발생할 확률이 높겠죠.\r\n\r\n<ColorText color='var(--primary)'>Storage problem</ColorText>  \r\n- 사용할 모든 n-gram의 수를 저장해야 하는 문제가 있죠.\r\n이는 n이 커지고, corpus가 클수록 모델 사이즈가 커진다는 것이죠.\r\n\r\nn-gram을 이용하여 아래와 같은 방식으로 문장을 생성할 수 있어요.\r\n\r\n![](assets/rnn/00.png)\r\n![](assets/rnn/01.png)\r\n\r\n놀랍게도 문법적으로는 틀리지 않지만, 문맥에 맞지 않는 내용이 만들어지죠.\r\n자연스럽게 문맥을 생성하려면 n을 키워야하고, 이는 모델 사이즈가 커지고 sparsity problem이 커지는 문제가 발생하죠.\r\n\r\n# RNN\r\n\r\n위 문제를 해결하기 위해서 언어 모델에 인공 신경망을 사용하는 방법이 등장했어요.\r\n\r\n![](assets/rnn/02.png)\r\n\r\nSparsity problem을 해결하고 모든 n-gram을 저장할 필요가 없어졌어요.\r\n하지만 window를 키우기 위해서는 $W$도 커져야 하기에 window를 충분히 크게 만들 수 없습니다.\r\n그래서 어떤 길이의 입력이든 처리할 수 있는 방법이 필요합니다.\r\n\r\n![](assets/rnn/03.png)\r\n\r\n`RNN`의 기본 아이디어는 입력에 대해 동일한 $W$를 사용하는 것이에요.\r\nRNN은 아래와 같은 특징을 가집니다.\r\n\r\n* 입력에 길이에 상관없이 처리할 수 있어요.\r\n* 이론적으로 $t$ 시점에서 정보는 이전 시점에서 모든 정보를 가지고 있어요.\r\n* 긴 입력이 들어와도 모델 사이즈는 커지지 않아요.\r\n* <ColorText color='var(--primary)'>연산 속도가 느립니다.</ColorText>\r\n* <ColorText color='var(--primary)'>실제로 오래 전 시점의 정보에 접근하긴 어렵습니다.</ColorText>\r\n\r\n## Training RNN\r\n\r\n![](assets/rnn/04.png)\r\n\r\n$$\r\n\\frac{\\partial J}{\\partial W} = \\frac{1}{T} \\sum_{t=1}^{T} J^{(t)} (\\theta) \\tag{1}\r\n$$\r\n\r\nRNN을 학습하기 위해서 모든 결과에 대해 loss를 계산하고 이를 평균내 전체 loss를 구합니다.\r\n하지만 실제로 우리가 다루는 입력은 문장이거나 어쩌면 문서가 될 수 있어요.\r\n이런 방식으로 전체 corpus의 loss를 구하는 작업은 큰 연산을 필요로 하게 되죠.\r\n이를 SGD와 같은 방식으로 어느정도 해결을 할 수 있겠죠.\r\n\r\n![](assets/rnn/10.png)\r\n혹은 전체 loss에 대해 계산하지 않고, 일부 길이마다 잘라 계산하는 형식으로 연산량을 줄일 수 있을거에요.\r\n\r\n![](assets/rnn/05.png)\r\n\r\n각 step마다 loss를 구하는 식은 아래와 같습니다.\r\n\r\n$$\r\nh^{(t)} = \\sigma (W_x x^{(t)} + W_h h^{(t-1)}) \\tag{2}\r\n$$\r\n\r\n$$\r\n\\frac{\\partial J^{(4)}}{\\partial W} = \r\n\\frac{\\partial J^{(4)}}{\\partial \\hat{y}^{(t)}} \\frac{\\partial \\hat{y}^{(t)}}{\\partial h^{(t)}}\r\n\\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} \\cdots\r\n\\frac{\\partial h^{(1)}}{\\partial W} \\tag{3}\r\n$$\r\n\r\n\r\n\r\n## Generate & Evaluate\r\n\r\nRNN을 이용하여 텍스트를 생성하는 과정은 다음과 같아요.\r\n\r\n![](assets/rnn/06.png)\r\n\r\nhidden state를 통해 예측한 단어를 다음 시점의 입력으로 사용하는 것이죠.\r\n이렇게 [EOS]라는 단어가 나올 때 까지 반복하여 문장을 생성하는 것입니다.\r\n이때 생성된 확률 분포에서 무작위로 단어를 선택하여 예측하는 `repeated sampling`을 사용합니다.\r\n이는 확률이 높은 단어가 자주 선택되겠지만, 낮은 확률을 가진 출력도 선택될 수 있어요.\r\n\r\n$$\r\nperplexity = \\prod_{t=1}^{T} (\\frac{1}{P_{LM} (x^(t+1) | x^(t), \\cdots, x^(1))}) ^ {1/T}\r\n$$\r\n\r\n이제 우리는 언어 모델의 평가 지표로 `perplexity`를 사용할거예요.\r\n언어 모델에서 이전 단어들이 주어졌을 때 다음 단어에 대한 확률의 역수를 모두 곱하여 $\\frac{1}{T}$승으로 normalization 한 것이죠.\r\n이는 cross-entrophy를 이용한 loss $J(\\theta)$의 exponential을 한 것과 같습니다.\r\n\r\n$$\r\n\\prod_{t=1}^{T} (\\frac{1}{P_{LM} (x^(t+1) | x^(t), \\cdots, x^(1))}) ^ {1/T} = \r\n\\prod_{t=1}^{T}(\\frac{1}{\\hat{y}_{x_{t+1}}^{(t)}})^{1/T} =\r\n\\exp (\\frac{1}{T} \\sum_{t=1}^{T} - \\log \\hat{y}_{x_{t+1}}^{(t)}) = \\exp (J(\\theta))\r\n$$\r\n\r\n우리는 loss가 낮아지도록 모델을 학습시켜요.\r\n즉 perplexity는 점수가 낮을수록 잘 학습된 모델이라고 할 수 있겠네요.\r\n\r\n뭐 문장 생성작업 외에도 entity recognition, text classification 등의 작업을 수행할 수 있어요.\r\n\r\n![](assets/rnn/07.png)\r\n![](assets/rnn/08.png)\r\n\r\n# LSTM, RNN Variants\r\n\r\n![](assets/rnn/09.png)\r\n\r\n우리는 학습을 위해 식 (3)과 같이 loss에 대한 기울기를 계산하였습니다.\r\n\r\n$$\r\n\\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} = \r\n\\frac{\\partial h^{(2)}}{\\partial h^{(1)}} \\times\r\n\\frac{\\partial h^{(3)}}{\\partial h^{(2)}} \\times\r\n\\cdots \\times \\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} \\tag{4}\r\n$$\r\n\r\n$$\r\n\\frac{\\partial h^{(t)}}{\\partial h^{(t-1)}} = W_h h^{(t-1)} \\tag{5}\r\n$$\r\n\r\n\r\n식 (3)을 풀어보면 식 (4)의 형태가 등장하는데 식 (5)를 활용하면 $W_h$가 여러번 곱해지는 형태를 가지는 것을 볼 수 있어요.\r\n만약 step $t$가 크면 클수록 $W_h$가 더 많이 곱해지는 모습이 보이겠죠.\r\n그런데 만약 $\\left| W_h \\right| > 1$ 이라면 step이 길어질수록 loss에 대한 $W_h$의 기울기는 점점 더 커질거에요.\r\n이 상태로 gradient descent를 한다면 minimum과 멀어지는 쪽으로 학습이 진행될 것입니다.\r\n이를 `exploding gradient`라고 합니다.\r\n이를 해결하기 위해서 기울기가 일정 크기 이상 커지면 적당한 값으로 clipping하는 방식으로 해결할 수 있을거예요.\r\n\r\n반대로 $\\left| W_h \\right| < 1$ 이라면 step이 길어질수록 loss에 대한 $W_h$의 기울기는 점점 더 작아질거에요.\r\n이를 `vanishing gradient`라고 합니다.\r\nloss를 계산할 때 너무 먼 step과의 기울기를 구하는 것은 문제가 발생하게 됩니다.\r\n\r\n## LSTM\r\n\r\nvanishing gradient 문제를 해결하기 위해 등장한 모델 구조가 `LSTM (Long Short-Term Memory Network)`입니다.\r\nLSTM은 기존 RNN에서 멀리 떨어진 정보에 대해 저장하는 `cell`이 추가된 형태에요.\r\n이 cell로부터 정보를 읽고, 쓰고, 지우고를 할 수 있어요.\r\n\r\n![](assets/rnn/11.png)\r\n\r\n기능을 천천히 살펴봅시다.\r\n\r\n![](assets/rnn/12.png)\r\n\r\n위 부분은 LSTM의 cell state 입니다.\r\n이는 정보가 저장되는 장소에요.\r\n\r\n![](assets/rnn/13.png)\r\n\r\nLSTM의 첫 단계는 cell state에 저장된 정보를 잊을지 보존할지 선택하는 경로에요.\r\n`forget gate`에서 $h_{t-1}$과 $x_t$를 $W_f$와 연산한 후 sigmoid를 통해 0과 1사이 값으로 만들어 줍니다.\r\n만약 이 값이 1이 되면 모든 정보를 보존하고 0이되면 모든 정보를 버리는 것이 되겠죠.\r\n실제로는 0~1 사이 값이 나올테니 일부 정보를 보존하고 버리는 식으로 진행되겠죠.\r\n\r\n$$\r\nf_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\r\n$$\r\n\r\n![](assets/rnn/14.png)\r\n\r\n다음 단계는 앞으로 들어오는 정보 중 어떤 것을 cell에 저장할 지 정하는 부분이에요.\r\n`input gate`라고 불리는 sigmoid가 어떤 값을 업데이트할지 정합니다.\r\n이를 tanh를 거친 정보와 곱해 cell에 전달합니다.\r\n\r\n$$\r\ni_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\r\n$$\r\n$$\r\n\\tilde{C}_t = tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\r\n$$\r\n\r\n![](assets/rnn/15.png)\r\n\r\nforget gate와 input gate에서 생성된 정보를 이용하여 cell state를 업데이트 할거에요.\r\n\r\n$$\r\nC_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\r\n$$\r\n\r\n![](assets/rnn/16.png)\r\n\r\n마지막으로 무엇을 output으로 보낼지 정하는 작업이 남아있어요.\r\n`out gate`에서 $h_{t-1}$과 $x_t$를 통해 어느 정보를 cell state에서 output으로 낼 것인지 결정합니다.\r\ncell state에 tanh를 거친 정보와 sigmoid와 곱해 결과를 만들어 내는 것이죠.\r\n\r\n$$\r\no_t = \\sigma (W_o [h_{t-1}, x_t] + b_o)\r\n$$\r\n$$\r\nh_t = o_t * tanh(C_t)\r\n$$\r\n\r\n이런 방식으로 우리는 오래된 정보를 기억하는 식으로 설계할 수 있습니다.\r\n만약 forget gate가 1이고 input gate가 0이면 오래된 정보도 영원히 남아있을 수 있죠.\r\n하지만 너무 깊은 neural network는 항상 optimization 문제가 발생했어요.\r\n이를 residual connection을 통해 해결하고는 합니다.\r\n\r\n## Bidirectional and Multi-layer RNN\r\n\r\n```\r\nI am _____ , and I can eat half a pig.\r\n```\r\n만약 우리는 위와 같이 문장의 빈칸에 어울릴 단어를 찾는 문제를 해결해야 한다면 어떻게 할까요?\r\n먼저 문장의 끝까지 보고 빈칸에 'hungry'라는 단어가 들어갈 수 있다고 생각할 것입니다.\r\n즉, 문장의 앞 뒤 정보를 아는 것은 단어를 선택하는 것에 있어 중요한 부분이라고 할 수 있죠.\r\n하지만 기존 RNN은 예측하고자 하는 단어 뒤에 있는 정보를 알 수 없었어요.\r\n이러한 문제를 해결하기 위해 `Bidirectional RNN`이 등장합니다.\r\n\r\n```\r\nThe movie was terribly exciting !\r\n```\r\n위 예시를 볼게요.\r\n우리는 이 문장의 감정을 분석하려고 합니다.\r\n중간에 terribly라는 단어가 있지만 뒤에 있는 exciting을 보면 이 단어는 긍정적으로 사용되는 것을 알 수 있어요.\r\n그래서 뒤에 있는 exciting의 정보를 앞으로 전달할 수 있는 모델을 설계할거에요.\r\n방법은 간단해요 2개의 레이어를 사용하는 것입니다.\r\n\r\n![](assets/rnn/17.png)\r\n\r\n하나는 문장의 앞에서 뒤로 정보를 전달하는 레이어, 다른 하나는 뒤에서 앞으로 정보를 전달하는 레이어죠.\r\n출력은 두 레이어의 정보를 합친 상태를 이용하여 output을 생산하는 방식이에요.\r\n\r\n$$\r\n\\overrightarrow{h}^{(t)} = RNN_{FW}(\\overrightarrow{h}^{(t-1)}, x^{(t)})\r\n$$\r\n$$\r\n\\overleftarrow{h}^{(t)} = RNN_{FW}(\\overleftarrow{h}^{(t-1)}, x^{(t)})\r\n$$\r\n$$\r\nh^{(t)} = [\\overrightarrow{h}^{(t)}; \\overleftarrow{h}^{(t)}]\r\n$$\r\n\r\nRNN은 사실 여러 state를 거치기 때문에 그 자체로도 이미 깊다고 할 수 있어요.\r\n이를 여러 레이어를 추가한 것이 `multiple RNN`입니다.\r\n이는 더 복잡한 문제를 해결하는 데 사용할 수 있죠.\r\n다른 말로는 `stacked RNN`이라고도 해요.\r\n\r\n![](assets/rnn/18.png)","tableOfContents":{"items":[{"url":"#n-gram-language-model","title":"N-Gram Language Model"},{"url":"#rnn","title":"RNN","items":[{"url":"#training-rnn","title":"Training RNN"},{"url":"#generate--evaluate","title":"Generate & Evaluate"}]},{"url":"#lstm-rnn-variants","title":"LSTM, RNN Variants","items":[{"url":"#lstm","title":"LSTM"},{"url":"#bidirectional-and-multi-layer-rnn","title":"Bidirectional and Multi-layer RNN"}]}]},"frontmatter":{"description":"RNNs and Language Models","title":"RNN"}},"file":{"modifiedTime":"2023-12-15T15:14:43.513Z"}},"pageContext":{"id":"2898c0d8-3411-59c7-b37b-313f27d9f966","relativePath":"nlp/rnn.mdx","frontmatter":{"title":"RNN","description":"RNNs and Language Models","order":3}}},"staticQueryHashes":["123912876","2317542362"],"slicesMap":{}}