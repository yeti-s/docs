{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-nlp-attention-mdx","path":"/c129aada-4663-525e-84cb-558d9a4fede4","result":{"data":{"mdx":{"id":"c129aada-4663-525e-84cb-558d9a4fede4","body":"\r\n우리는 seq2seq 모델을 통해 MT 작업을 수행하는 것을 보았습니다.\r\n그런데 여기에 한 가지 문제가 있어요.\r\n입력의 모든 정보가 인코더의 마지막 레이어에 저장되어야 한다는 것이죠.\r\n그로 인해 정보가 압축되는 현상이 발생해요.\r\n이를 `bottleneck problem`이라고 합니다.\r\n\r\n# Attention\r\n\r\n이를 출력을 생성하는 단계에서 인코더의 모든 step에 대한 정보와 접근하여 출력을 결정하는 `attention` 메카니즘을 통해 해결했습니다.\r\n\r\n![](assets/attention/00.png)\r\n\r\n","tableOfContents":{"items":[{"url":"#attention","title":"Attention"}]},"frontmatter":{"description":"Machine translation, sequence to sequence","title":"Attention"}},"file":{"modifiedTime":"2023-12-15T09:21:00.121Z"}},"pageContext":{"id":"c129aada-4663-525e-84cb-558d9a4fede4","relativePath":"nlp/attention.mdx","frontmatter":{"title":"Attention","description":"Machine translation, sequence to sequence","order":5}}},"staticQueryHashes":["123912876","2317542362"],"slicesMap":{}}