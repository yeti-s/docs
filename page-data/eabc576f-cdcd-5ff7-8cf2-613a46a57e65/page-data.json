{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-rl-markov-mdx","path":"/eabc576f-cdcd-5ff7-8cf2-613a46a57e65","result":{"data":{"mdx":{"id":"eabc576f-cdcd-5ff7-8cf2-613a46a57e65","body":"\r\nimport ColorText from '@contents/components/ColorText';\r\n\r\n공부를 시작하기에 앞서 먼저 우리가 해결해야 할 문제에 대해 목표를 정하고 가봅시다.\r\n`Path-Finding In Grid` 라는 문제를 풀어보려고 합니다.\r\n\r\n![](assets/markov/00.png)\r\n\r\n먼저 회색이 아닌 랜덤한 셀에서 시작하여 회색인 셀로 도달하는 경로를 찾으려고 합니다.\r\n각 Action은 위, 아래, 왼쪽, 오른쪽 이동이 전부이고요.\r\n목표에 도달하기 위한 Policy를 찾는 것이 우리의 목적이죠.\r\n\r\n# MDP(Markov Decision Process)\r\n\r\n`MDP`는 상태 $S_t$, 확률 밀도 함수 $P$, 보상 함수 $r(S_t, a_t)$로 이루어진 모델입니다.\r\n순차적으로 행동을 결정해야 하는 문제를 풀기 위한 수학 모델이죠.\r\n거의 모든 퍼즐 문제를 이를 활용하여 풀 수 있어요!\r\n\r\n![](assets/markov/01.png)\r\n\r\n위 그림과 같이 Agent가 길을 찾는 작업을 수행한다고 할 때, 해당 작업에 대한 State와 State의 관계를 오른쪽 그림과 같이 정의할 수 있습니다.\r\n여기서는 State와 State 사이 가중치를 가지는 관계를 형성하고 있죠.\r\n이를 Transition Matrix로 표현할 수도 있어요.\r\n\r\n$$\r\np_{ij} = P(S_{t+1} = s_j | S_t = s_i) \\tag{1}\r\n$$\r\n\r\n$$\r\nP(S_{t+1} = s_j) = \\sum_{i=1}^{n} P(S_t = s_i) \\cdot p_{ij} \\tag{2}\r\n$$\r\n\r\n식 (1)과 같이 $t$ 시점에서 $s_i$에서 $s_j$로 가는 확률을 $p_{ij}$라고 한다면, 식 (2)와 같이 $t+1$ 시점에서 $s_j$에 도달할 확률은 앞선 State들에 대한 기대값의 합이라고 할 수 있습니다.\r\n물론 <ColorText color='var(--error)'>확률 밀도 함수 $P$에 대한 정보를 가지고 있어야 하기 떄문에 환경을 완전히 관찰할 수 있어야 한다는 조건</ColorText>이 필요합니다.\r\n\r\n각 상태와 확률 분포$(S, P, R, \\gamma)$에 대한 가치 평가는 다음과 같이 이루어집니다.\r\n$$\r\nr(s_i) = E[R_{t+1}|S_t = s_i]\r\n$$\r\n\r\n","tableOfContents":{"items":[{"url":"#mdpmarkov-decision-process","title":"MDP(Markov Decision Process)"}]},"frontmatter":{"description":"Markov Decision Process에 대해 소개합니다.","title":"Markov Decision Process","date":"2024년 1월 9일"}}},"pageContext":{"id":"eabc576f-cdcd-5ff7-8cf2-613a46a57e65","relativePath":"rl/markov.mdx","frontmatter":{"title":"Markov Decision Process","description":"Markov Decision Process에 대해 소개합니다.","subject":"기초","visible":true,"order":2,"date":"2024년 1월 9일"}}},"staticQueryHashes":["2317542362","2468140611"],"slicesMap":{}}