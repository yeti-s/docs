{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-paper-tuning-lo-ra-lo-ra-mdx","path":"/Tuning/LoRA","result":{"data":{"mdx":{"id":"8f0b73a2-528b-505a-af80-f5541d38b00e","body":"\r\nimport * as Elem from '@elements';\r\n\r\n기존에 효과적인 Fine-Tuning 방법이라고만 대략적으로 알고 있었던 `LoRA`에 대해 자세하게 논문을 읽어보려 합니다.\r\n\r\n# Introduction \r\n\r\n최근에 NLP의 추세는 거대한 사전 학습 모델을 이용하여 여러 Down-Stream Task에 적용하는 것입니다.\r\n보통은 기존 모델의 모든 파라미터를 Fine-Tuning을 이용하여 특정 작업에 효과적으로 만들고 있죠.\r\n\r\n그런데 이를 몇몇 파라미터만을 적용하여 학습을 진행하거나, 외부 모델을 붙여 작업에 적용하는 방법으로 적용할 수 있다고 보았습니다.\r\n현재까지 제시된 방법은 모델의 추론 속도를 늦추거나, 모델의 가용한 시퀀스 길이를 줄이는 효율과 성능의 Trade-Off 관계를 가집니다.\r\n\r\n<Elem.TwoCols>\r\n<Elem.Cols size={30}>\r\n![Figure 1](figure1.png)\r\n</Elem.Cols>\r\n<Elem.Cols size={60}>\r\n접근을 다르게 하여 작업에 비해 너무 많은 파라미터를 가진 모델은 실제로 낮은 Intrinsic Dimension을 가지고 있다는 발견을 기반으로 \r\n논문에서는 `LoRA(Low-Rank Adaptation)`을 제안합니다.\r\n`LoRA`는 사전 학습된 모델의 파라미터에 대해서는 고정을 시킨후 추가적인 파라미터를 통해 간접적으로 모델을 학습합니다.\r\n</Elem.Cols>\r\n</Elem.TwoCols>\r\n\r\n`LoRA`는 아래와 같은 이점이 있습니다.\r\n* 사전 학습 모델은 파라미터를 공유한 채 많은 `LoRA` 모듈을 만들어낼 수 있습니다.  \r\n크기가 작은 `LoRA` 모듈을 교체하는 것만으로 다양한 작업을 수행할 수 있죠.\r\n* 대다수의 파라미터를 고정하고 Low-Rank 행렬에 대한 최적화를 진행하므로 최대 3배까지 효과적으로 학습을 진행할 수 있습니다.\r\n* 배포시 `LoRA` 모듈을 고정된 파라미터에 합칠 수 있어 기존 모델 대비 추론 속도에 영향이 없습니다.\r\n* LoRA prefix-tuning\r\n<Elem.Comment> 4번째 항목은 다시 보고 정리</Elem.Comment>\r\n\r\n# Problem Statement\r\n\r\nFull Fine-Tuning 에서 업데이트되는 파라미터 $\\Delta \\Phi$의 크기는 기존 모델의 파라미터 $\\Phi_0$와 같습니다. \r\nFine-Tuning은 아래와 같은 Objective Function을 따르죠.\r\n\r\n$$\r\n\\max_{\\Phi} \\sum_{(x,y) \\in Z} \\sum_{t=1}^{\\left|y\\right|} \\log (P_{\\Phi} (y_t | x, y_{<t})) \\tag{1}\r\n$$\r\n\r\n논문에서 제시하는 방법은 훨씬 적은 파라미터 $\\Theta$에 대해 업데이트를 진행합니다. \r\n학습되는 파라미터의 크기 $\\Delta \\Phi$는 $\\Delta \\Phi (\\Theta)$와 같습니다.\r\n이를 통해 Fine-Tuning을 진행하면 아래와 같은 Objective Function을 따르죠.\r\n\r\n$$\r\n\\max_{\\Theta} \\sum_{(x,y) \\in Z} \\sum_{t=1}^{\\left|y\\right|} \\log (p_{{\\Phi_0} + \\Delta \\Phi (\\Theta)} (y_t | x, y_{<t}))\r\n$$\r\n\r\n# Our Method\r\n\r\n일반적으로 Full-Rank 행렬로 이루어져 있지만, 사전 학습된 언어 모델은 Low Instrisic Dimension을 가지고 더 적은 차원으로 효과적으로 학습시킬 수 있다고 가정합니다.\r\n사전 학습 모델의 Weight $W_0 \\in R^{d \\times k}$\r\n\r\n\r\n\r\n","tableOfContents":{"items":[{"url":"#introduction","title":"Introduction"},{"url":"#problem-statement","title":"Problem Statement"},{"url":"#our-method","title":"Our Method"}]},"frontmatter":{"description":"Low-Rank Adaptation of Large Language Models","title":"LoRA","date":"2024년 3월 19일"}}},"pageContext":{"id":"8f0b73a2-528b-505a-af80-f5541d38b00e","frontmatter":{"title":"LoRA","description":"Low-Rank Adaptation of Large Language Models","date":"2024년 3월 19일","subject":"Tuning","visible":true}}},"staticQueryHashes":["1569657708","2317542362"],"slicesMap":{}}