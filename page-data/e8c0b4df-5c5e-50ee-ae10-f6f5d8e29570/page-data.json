{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-nlp-multilinguality-mdx","path":"/e8c0b4df-5c5e-50ee-ae10-f6f5d8e29570","result":{"data":{"mdx":{"id":"e8c0b4df-5c5e-50ee-ae10-f6f5d8e29570","body":"\r\n지금까지 과정에서 우리는 대부분 영어를 사용한 모델들의 예시를 보았어요.\r\n하지만 세상에는 어마어마한 수의 언어가 존재하죠.\r\n우리는 사전 학습 방법으로 예상치 못한 엄청난 성능 향상을 경험했죠.\r\n이 방법이 파라미터를 공유하는 다양한 언어를 다루는 모델에서 통할까요?\r\n\r\n# Multilingual language model\r\n\r\n사실 multilinualism에 대한 특별한 학습 방법 같은 것은 없어요.\r\n단지 다양한 언어에 대한 텍스트를 활용하여 사전 학습을 하는 것이죠.\r\n또 모든 언어를 포괄하는 공통의 vocabulary를 사용하는 것 뿐이죠.\r\n\r\n우리가 관찰해야 하는 특징은 두 가지입니다.\r\n* `Language typology` : 다른 언어에서 특정한 차이를 multilingual model이 포착할 수 있을까? \r\n* `Language universals` : multilingual model이 언어간 공통 구조를 포착할 수 있을까?  \r\n대부분의 언어들이 공통적으로 주어 동사 목적어등의 관계를 가지고 있어요.\r\n\r\n\r\n![](assets/multilingual/00.png)\r\n\r\n위 그림은 스페인어를 한번도 보지 못한 모델이 스페인어에 대한 syntatic 이해를 하고 있는 것을 보여줍니다.\r\n\r\n하지만 다양한 언어에 대한 데이터의 퀄리티도 다 다를 것입니다.\r\n\r\n![](assets/multilingual/01.png)\r\n\r\n그래서 우리는 위와 같은 방식으로 fine-tuning을 진행합니다.\r\n\r\n# Cross-lingual transfer (XLT)\r\n\r\n언어 모델을 A 언어를 이용한 어떠한 작업에 대해 fine-tuning 하고, 추가적인 학습 없이 B 언어를 이용해 그 작업을 수행한다면?\r\n\r\n![](assets/multilingual/02.png)\r\n\r\n학습한 언어와 다른 언어로 테스트를 진행해도 결과가 막 나쁘지는 않네요.\r\n\r\n# Conclusion\r\n\r\nMultilingual model은 다양한 언어에 대한 다양성과 특정성을 보존하며 language-generality의 균형을 잘 맞춰야 합니다.\r\n","tableOfContents":{"items":[{"url":"#multilingual-language-model","title":"Multilingual language model"},{"url":"#cross-lingual-transfer-xlt","title":"Cross-lingual transfer (XLT)"},{"url":"#conclusion","title":"Conclusion"}]},"frontmatter":{"description":"Multilinguality","title":"Multilinguality"}},"file":{"modifiedTime":"2023-12-17T16:53:13.399Z"}},"pageContext":{"id":"e8c0b4df-5c5e-50ee-ae10-f6f5d8e29570","relativePath":"nlp/multilinguality.mdx","frontmatter":{"title":"Multilinguality","description":"Multilinguality","order":13}}},"staticQueryHashes":["123912876","2317542362"],"slicesMap":{}}