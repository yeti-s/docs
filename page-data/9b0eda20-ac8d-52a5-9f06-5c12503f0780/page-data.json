{"componentChunkName":"component---src-components-templates-main-template-tsx-content-file-path-contents-quantization-awq-mdx","path":"/9b0eda20-ac8d-52a5-9f06-5c12503f0780","result":{"data":{"mdx":{"id":"9b0eda20-ac8d-52a5-9f06-5c12503f0780","body":"\r\n# Paper\r\n[AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/pdf/2306.00978.pdf) 논문을 바탕으로 작성하였습니다.\r\n\r\n\r\n## Introduction\r\nQuantization-Aware Training (QAT)은 높은 비용 때문에 현실적으로 어려움이 있고, Post-Training Quantization (PTQ)는 낮은 비트에서 크게 정확도가 하락하는 문제가 있어요.\r\nGPTQ 처럼 에러를 보정하기 위한 추가적인 정보를 주는 방버은 과적합 되거나 특징들을 왜곡시킬 수 있습니다.\r\n이러한 문제를 해결하기 위해 Activation-aware Weight Quantization (AWQ)를 소개합니다.\r\n\r\nLLM을 관찰해보니 몇몇(0.1%~1%) weight가 돌출되어 있는 현상을 발견하였어요.\r\n이런 weight을 제외하고 quantization을 진행하니 에러가 크게 감소하는 것을 보았습니다.\r\n우리는 이런 돌출 구간을 activation의 분포를 통해 찾을 수 있었답니다.\r\nactivation에서 높은 값을 가지는 채널이 더 중요한 특징들을 다루기 때문에 거기에 해당하는 weight가 돌출되어 있다는 사실을 보았어요.\r\n그래서 우리는 에러를 줄이는 최적의 값을 찾기 위한 채널별 scaling을 구상했습니다.\r\n\r\n![Figure 1](assets/awq/00.PNG)\r\n\r\n이 방법은 backpropagation이나 reconstruction을 하지 않기 때문에 모델의 특징을 유지하며 과적합 문제가 발생하지 않아요.\r\n또한 우리는 메모리 사용량과 추론시 오버헤드를 최소화 하는 등 장점을 가지게 될 것입니다.\r\n\r\n\r\n## AWQ: Activation-aware Weight Quantization\r\n","tableOfContents":{"items":[{"url":"#paper","title":"Paper","items":[{"url":"#introduction","title":"Introduction"},{"url":"#awq-activation-aware-weight-quantization","title":"AWQ: Activation-aware Weight Quantization"}]}]},"frontmatter":{"description":"Activation-aware Weight Quantization for LLM Compression and Acceleration","title":"AWQ"}},"file":{"modifiedTime":"2023-12-11T16:10:21.427Z"}},"pageContext":{"id":"9b0eda20-ac8d-52a5-9f06-5c12503f0780","relativePath":"quantization/awq.mdx","frontmatter":{"title":"AWQ","description":"Activation-aware Weight Quantization for LLM Compression and Acceleration","order":3}}},"staticQueryHashes":["2317542362","3731189812"],"slicesMap":{}}