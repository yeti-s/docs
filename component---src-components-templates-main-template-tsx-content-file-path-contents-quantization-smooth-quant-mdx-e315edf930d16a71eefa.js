"use strict";(self.webpackChunkyeti_docs=self.webpackChunkyeti_docs||[]).push([[6084],{7094:function(e,a,t){var n=t(4316),l=(t(7294),t(917));const s=(0,n.Z)("div",{target:"e1k40o190"})("display:inline;color:",(e=>e.color),";");a.Z=e=>{let{color:a,children:t}=e;return(0,l.tZ)(s,{color:a},t)}},3832:function(e,a,t){var n=t(4316),l=(t(7294),t(917));const s=(0,n.Z)("code",{target:"e1h9msy90"})({name:"15yct12",styles:"display:block;width:100%;padding:0.2rem 0.4rem 0rem;color:var(--comment-color)"});a.Z=e=>{let{children:a}=e;return(0,l.tZ)(s,null,a)}},961:function(e,a,t){t.r(a),t.d(a,{default:function(){return I}});var n=t(1151),l=t(7294),s=(t(7094),t(3832));function m(e){const a=Object.assign({p:"p",a:"a",h1:"h1",strong:"strong",br:"br",span:"span",ol:"ol",li:"li",em:"em",math:"math",semantics:"semantics",mrow:"mrow",mi:"mi",annotation:"annotation",div:"div",msub:"msub",mo:"mo",mn:"mn",mtext:"mtext",msup:"msup",mover:"mover",h2:"h2",pre:"pre",code:"code"},(0,n.ah)(),e.components);return l.createElement(l.Fragment,null,l.createElement(a.p,null,l.createElement(a.a,{href:"https://arxiv.org/abs/2211.10438"},"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models")," 논문을 바탕으로 작성하였습니다."),"\n",l.createElement(a.h1,{id:"introduction"},"Introduction"),"\n",l.createElement(a.p,null,"Large-scale language model(LLM)은 다양한 분야에서 뛰어난 성과를 내고 있다.\r\n하지만 큰 모델 사이즈로 인해 LLM 서비스를 제공하는 것에는 큰 비용이 든다.\r\n예를 들어 GPT-3 모델은 175B 파라미터가 존재하고 FP16 형태로 실행하기 위해 350GB의 메모리가 필요하다.\r\n따라서 weight와 activation을 적은 비트로 quantization하는 방법으로 GPU 요구량을 줄이고 계산을 가속화할 수 있다."),"\n",l.createElement(a.p,null,"그러나 CNN모델 또는 BERT와 같은 작은 transformer 모델과 달리 LLM의 activation을 quantization하는 것은 쉽지 않다.\r\n6.7B 이상의 LLM 모델은 activation에 큰 규모의 시스템적 outliers들이 등장해서 quantization error를 증가시키고 정확도를 감소시킨다.\r\n",l.createElement(a.a,{href:"https://arxiv.org/pdf/2206.01861.pdf"},l.createElement(a.strong,null,"ZeroQuant")),"는 GPT-3-350M, GPT-J-6B 모델에서 좋은 성능을 냈지만 OPT-175B에서는 정확도를 유지하지 못했다.\r\n",l.createElement(a.a,{href:"https://arxiv.org/pdf/2208.07339.pdf"},l.createElement(a.strong,null,"LLM.int8")),"는 하드웨어 가속기에 효과적으로 decomposition을 구현하기 어렵다.\r\n그러므로 효과적, 하드웨어 친화적, 그리고 훈련이 필요 없는 quantization 방법이 필요하다."),"\n",l.createElement(s.Z,null,l.createElement(a.p,null,"BERT와 같은 작은 transformer는 quantization이 쉬운 것일까?\r\nZeroQuant가 GPT와 OPT에서 효율이 다른 원인은 무엇인가?")),"\n",l.createElement(a.p,null,"SmoothQuant는 서로 다른 토큰에서 채널에 대한 비슷한 분포로 outlier가 존재한다는 관찰을 기반으로 한다.\r\nSmoothQuant는 quantization의 어려움을 activation에서 weight로 offline하게 이동시켰다."),"\n",l.createElement(s.Z,null,l.createElement(a.p,null,"activation의 quantization이 더 어렵다는 분석 아래에 적혀 있지만 인용한 LLM.int8 논문 추가로 살펴보자.\r\n찾아보니 그냥 분포를 관찰한 것이었다. 이유는 따로 없음!")),"\n",l.createElement(a.h1,{id:"review-of-quantization-difficulty"},"Review of Quantization Difficulty"),"\n",l.createElement(a.p,null,"LLM의 quantization을 어렵게 하는 activation outlier의 몇몇 패턴을 관측하였다.",l.createElement(a.br),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 704px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/09baf853f0a8a4f90786702e29fd0c47/3fca6/00.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 31.818181818181817%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABkElEQVR42iWRvW/TUBTF84+wsDMhderMhBiYEEsnJhADAyAxwJIJJgY2BHslYEGlEIRaVIoQJaHULXKpazexkziOP/Li5y85P26dO7x377nSOefe22JBE4lhkqmUfFETJwlFUTCfp3ieh2EY2LbNbKZQSqG1bvp1XVOWJXm+zM+jdf701zfYvLhK4A2wPm3j/D4gKwt0GhNFAUEwIU1FLNMiEgk2lX8umBZMkQoWx2Ej0lJC+OvOA7qXLzF89YKNCysExiGzvOLrd4+DoxG+H1EWGssO6Ww5HJ+MhDShKjU/ej4ft/rS88VpTktTs9N+yfraQ9pXHvFl5Soqjniz6XLzbpdb97sYf31xkPDk6T7X1na493gP1x1zaAbcuL3H6vVd2s8Nca2WI/t+yLvPDq+fdQiOTErB3MGE7W8Ouz9d2ZOWveWcWEM+dCx6f1wZLyVJNL39Pm/fn4rDMVWVLwm1MA+dY/quTV5VzXInoS+1JU7OyLKswcZTD/vMZBr4TR2nIaeeyWD0T47nymEW/AezBKyVHNWbdgAAAABJRU5ErkJggg==\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="Figure 4"\n        title=""\n        src="/static/09baf853f0a8a4f90786702e29fd0c47/5ebd7/00.png"\n        srcset="/static/09baf853f0a8a4f90786702e29fd0c47/06437/00.png 176w,\n/static/09baf853f0a8a4f90786702e29fd0c47/ba1c3/00.png 352w,\n/static/09baf853f0a8a4f90786702e29fd0c47/5ebd7/00.png 704w,\n/static/09baf853f0a8a4f90786702e29fd0c47/fd84e/00.png 1056w,\n/static/09baf853f0a8a4f90786702e29fd0c47/3fca6/00.png 1112w"\n        sizes="(max-width: 704px) 100vw, 704px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}})),"\n",l.createElement(a.ol,null,"\n",l.createElement(a.li,null,"activation이 weight보다 quantization하기 어렵다.",l.createElement(a.br),"\n","weight의 분포는 꽤 평평하여 INT8 심지어 INT4로 quantize 하여도 정확도가 감소하지 않았다."),"\n",l.createElement(a.li,null,"outlier가 activation quantization을 어렵게 한다.\r\nactivation의 outlier는 다른 값보다 ~100배 더 크게 관측되었다.\r\n이러한 경우에 per-tensor quantization을 진행한다면 outlier에 의해 큰 quantization error를 만든다."),"\n",l.createElement(a.li,null,"outlier는 고정된 channel에 나타난다.\r\n만약 한 채널에 outlier가 있었다면, 해당 채널은 모든 토큰에서 outlier를 가진다.\r\n그래서 per-channel quantization을 per-tensor quantization보다 더 작은 quantization error를 가지게 된다.\r\n하지만 per-channel quantization은 ",l.createElement(a.em,null,l.createElement(a.strong,null,"GEMM"))," 커널에 적합하지 않다.",l.createElement(a.br),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 530px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/d4d6fd03970823e88995de374281b5fe/b6a9b/01.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 32.38636363636364%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABJ0lEQVR42kVQXZOCMAzk//80Xw5xFAQpSqF8tQUU0Qdu9pLozD0wDZvN7iaBMQZh+IM0y3A+nxHHMXa7Hfq+h9Yl/SdYlgWn45GwARVh4T7C87lgH4aoqhpd22IfRZimCYG1FkmSwDmPeZ7pu+N+n0VEa00DlfSuRQHmGlPjerthHEfkeY6WxBhXSpHJEwEnOZK7dU5EHo+HNNgtPp1QFAq60pKeDcqyhCJxNkrTVF7W4FA8GzSNQURxD4cDMiLwa5oW27aJiFI53HcLTlOWNxHy3uNyuaCuazRNI9yVE1o7SPQ0zVCzKzX8OGH73SQNkzlt8V15GAa5G5+HsbbrZH2tK7xebwQc07oP0VpH93Lo+k4ScO29+69pkHkf3IsBi7Eh1+u64g/ZzbsPoi/EPwAAAABJRU5ErkJggg==\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="Table 1"\n        title=""\n        src="/static/d4d6fd03970823e88995de374281b5fe/b6a9b/01.png"\n        srcset="/static/d4d6fd03970823e88995de374281b5fe/06437/01.png 176w,\n/static/d4d6fd03970823e88995de374281b5fe/ba1c3/01.png 352w,\n/static/d4d6fd03970823e88995de374281b5fe/b6a9b/01.png 530w"\n        sizes="(max-width: 530px) 100vw, 530px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}})),"\n"),"\n",l.createElement(s.Z,null,l.createElement(a.p,null,"GEMM 커널이 뭔지 찾아보자.")),"\n",l.createElement(a.h1,{id:"smoothquant"},"SmoothQuant"),"\n",l.createElement(a.p,null,"per-channel quantization 대신 smoothing factor ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.mi,null,"s")),l.createElement(a.annotation,{encoding:"application/x-tex"},"s")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.4306em"}}),l.createElement(a.span,{className:"mord mathnormal"},"s"))))),"를 이용하여 linear layer의 동일한 수학적 결과를 만든다.\r\nquantization error를 줄이기 위해 모든 channel을 동일한 규모로 만든다."),"\n",l.createElement(a.div,{className:"math math-display"},l.createElement(a.span,{className:"katex-display"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"s"),l.createElement(a.mi,null,"j")),l.createElement(a.mo,null,"="),l.createElement(a.mi,null,"m"),l.createElement(a.mi,null,"a"),l.createElement(a.mi,null,"x"),l.createElement(a.mo,{stretchy:"false"},"("),l.createElement(a.mrow,null,l.createElement(a.mo,{fence:"true"},"∣"),l.createElement(a.msub,null,l.createElement(a.mi,null,"X"),l.createElement(a.mi,null,"j")),l.createElement(a.mo,{fence:"true"},"∣")),l.createElement(a.mo,{stretchy:"false"},")"),l.createElement(a.mo,{separator:"true"},","),l.createElement(a.mi,null,"j"),l.createElement(a.mo,null,"="),l.createElement(a.mn,null,"1"),l.createElement(a.mo,{separator:"true"},","),l.createElement(a.mn,null,"2"),l.createElement(a.mo,{separator:"true"},","),l.createElement(a.mo,null,"⋯"),l.createElement(a.mtext,null," "),l.createElement(a.mo,{separator:"true"},","),l.createElement(a.msub,null,l.createElement(a.mi,null,"C"),l.createElement(a.mi,null,"j"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"s_j=max(\\left| X_j \\right|), j=1,2,\\cdots,C_j")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.7167em",verticalAlign:"-0.2861em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span)))))),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}}),l.createElement(a.span,{className:"mrel"},"="),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}})),l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"1.0361em",verticalAlign:"-0.2861em"}}),l.createElement(a.span,{className:"mord mathnormal"},"ma"),l.createElement(a.span,{className:"mord mathnormal"},"x"),l.createElement(a.span,{className:"mopen"},"("),l.createElement(a.span,{className:"minner"},l.createElement(a.span,{className:"mopen delimcenter",style:{top:"0em"}},"∣"),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"}},"X"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"-0.0785em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span)))))),l.createElement(a.span,{className:"mclose delimcenter",style:{top:"0em"}},"∣")),l.createElement(a.span,{className:"mclose"},")"),l.createElement(a.span,{className:"mpunct"},","),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.1667em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.05724em"}},"j"),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}}),l.createElement(a.span,{className:"mrel"},"="),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}})),l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.9694em",verticalAlign:"-0.2861em"}}),l.createElement(a.span,{className:"mord"},"1"),l.createElement(a.span,{className:"mpunct"},","),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.1667em"}}),l.createElement(a.span,{className:"mord"},"2"),l.createElement(a.span,{className:"mpunct"},","),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.1667em"}}),l.createElement(a.span,{className:"minner"},"⋯"),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.1667em"}}),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.1667em"}}),l.createElement(a.span,{className:"mpunct"},","),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.1667em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.07153em"}},"C"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"-0.0715em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span))))))))))),"\n",l.createElement(a.p,null,"activation에 ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"s"),l.createElement(a.mi,null,"j"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"s_j")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.7167em",verticalAlign:"-0.2861em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span)))))))))),"를 나누어 모든 channel이 동일한 규모로 만들어 quantization error를 줄인다.\r\nactivation의 범위는 동적이기 때문에 몇몇의 calibration sample을 통해 scale을 추정해야 한다."),"\n",l.createElement(a.div,{className:"math math-display"},l.createElement(a.span,{className:"katex-display"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.mi,null,"Y"),l.createElement(a.mo,null,"="),l.createElement(a.mo,{stretchy:"false"},"("),l.createElement(a.mi,null,"X"),l.createElement(a.mi,null,"d"),l.createElement(a.mi,null,"i"),l.createElement(a.mi,null,"a"),l.createElement(a.mi,null,"g"),l.createElement(a.mo,{stretchy:"false"},"("),l.createElement(a.mi,null,"s"),l.createElement(a.msup,null,l.createElement(a.mo,{stretchy:"false"},")"),l.createElement(a.mrow,null,l.createElement(a.mo,null,"−"),l.createElement(a.mn,null,"1"))),l.createElement(a.mo,{stretchy:"false"},")"),l.createElement(a.mo,null,"⋅"),l.createElement(a.mo,{stretchy:"false"},"("),l.createElement(a.mi,null,"d"),l.createElement(a.mi,null,"i"),l.createElement(a.mi,null,"a"),l.createElement(a.mi,null,"g"),l.createElement(a.mo,{stretchy:"false"},"("),l.createElement(a.mi,null,"s"),l.createElement(a.mo,{stretchy:"false"},")"),l.createElement(a.mi,null,"W"),l.createElement(a.mo,{stretchy:"false"},")"),l.createElement(a.mo,null,"="),l.createElement(a.mover,{accent:"true"},l.createElement(a.mi,null,"X"),l.createElement(a.mo,null,"^")),l.createElement(a.mover,{accent:"true"},l.createElement(a.mi,null,"Y"),l.createElement(a.mo,null,"^"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"Y=(Xdiag(s)^{-1})\\cdot(diag(s)W)=\\hat{X}\\hat{Y}")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.6833em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"}},"Y"),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}}),l.createElement(a.span,{className:"mrel"},"="),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}})),l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"1.1141em",verticalAlign:"-0.25em"}}),l.createElement(a.span,{className:"mopen"},"("),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"}},"X"),l.createElement(a.span,{className:"mord mathnormal"},"d"),l.createElement(a.span,{className:"mord mathnormal"},"ia"),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g"),l.createElement(a.span,{className:"mopen"},"("),l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"mclose"},l.createElement(a.span,{className:"mclose"},")"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.8641em"}},l.createElement(a.span,{style:{top:"-3.113em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mtight"},l.createElement(a.span,{className:"mord mtight"},"−"),l.createElement(a.span,{className:"mord mtight"},"1"))))))))),l.createElement(a.span,{className:"mclose"},")"),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2222em"}}),l.createElement(a.span,{className:"mbin"},"⋅"),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2222em"}})),l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),l.createElement(a.span,{className:"mopen"},"("),l.createElement(a.span,{className:"mord mathnormal"},"d"),l.createElement(a.span,{className:"mord mathnormal"},"ia"),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"}},"g"),l.createElement(a.span,{className:"mopen"},"("),l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"mclose"},")"),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"}},"W"),l.createElement(a.span,{className:"mclose"},")"),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}}),l.createElement(a.span,{className:"mrel"},"="),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}})),l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.9468em"}}),l.createElement(a.span,{className:"mord accent"},l.createElement(a.span,{className:"vlist-t"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.9468em"}},l.createElement(a.span,{style:{top:"-3em"}},l.createElement(a.span,{className:"pstrut",style:{height:"3em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"}},"X")),l.createElement(a.span,{style:{top:"-3.2523em"}},l.createElement(a.span,{className:"pstrut",style:{height:"3em"}}),l.createElement(a.span,{className:"accent-body",style:{left:"-0.1667em"}},l.createElement(a.span,{className:"mord"},"^"))))))),l.createElement(a.span,{className:"mord accent"},l.createElement(a.span,{className:"vlist-t"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.9468em"}},l.createElement(a.span,{style:{top:"-3em"}},l.createElement(a.span,{className:"pstrut",style:{height:"3em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"}},"Y")),l.createElement(a.span,{style:{top:"-3.2523em"}},l.createElement(a.span,{className:"pstrut",style:{height:"3em"}}),l.createElement(a.span,{className:"accent-body",style:{left:"-0.25em"}},l.createElement(a.span,{className:"mord"},"^")))))))))))),"\n",l.createElement(a.p,null,"동일한 수학적 계산을 위해 weight에 앞서 구한 ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"s"),l.createElement(a.mi,null,"j"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"s_j")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.7167em",verticalAlign:"-0.2861em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span)))))))))),"를 곱해준다.\r\n하지만 이러한 방법은 quantization error를 activation에서 weight로 이동시키는 역할을 할 뿐이다.\r\n그래서 quantization difficulty를 weight와 activation이 적절하게 부담할 수 있도록 할 필요가 있다."),"\n",l.createElement(a.p,null,"quantization difficulty를 weight와 activation이 부담하는 정도를 결정하기 위해서 hyper-parameter ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.mi,null,"α")),l.createElement(a.annotation,{encoding:"application/x-tex"},"\\alpha")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.4306em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"}},"α"))))),"를 추가한다"),"\n",l.createElement(a.div,{className:"math math-display"},l.createElement(a.span,{className:"katex-display"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"s"),l.createElement(a.mi,null,"j")),l.createElement(a.mo,null,"="),l.createElement(a.mi,null,"m"),l.createElement(a.mi,null,"a"),l.createElement(a.mi,null,"x"),l.createElement(a.mo,{stretchy:"false"},"("),l.createElement(a.mrow,null,l.createElement(a.mo,{fence:"true"},"∣"),l.createElement(a.msub,null,l.createElement(a.mi,null,"X"),l.createElement(a.mi,null,"j")),l.createElement(a.mo,{fence:"true"},"∣")),l.createElement(a.msup,null,l.createElement(a.mo,{stretchy:"false"},")"),l.createElement(a.mi,null,"α")),l.createElement(a.mi,{mathvariant:"normal"},"/"),l.createElement(a.mi,null,"m"),l.createElement(a.mi,null,"a"),l.createElement(a.mi,null,"x"),l.createElement(a.mo,{stretchy:"false"},"("),l.createElement(a.mrow,null,l.createElement(a.mo,{fence:"true"},"∣"),l.createElement(a.msub,null,l.createElement(a.mi,null,"W"),l.createElement(a.mi,null,"j")),l.createElement(a.mo,{fence:"true"},"∣")),l.createElement(a.msup,null,l.createElement(a.mo,{stretchy:"false"},")"),l.createElement(a.mrow,null,l.createElement(a.mn,null,"1"),l.createElement(a.mo,null,"−"),l.createElement(a.mi,null,"α")))),l.createElement(a.annotation,{encoding:"application/x-tex"},"s_j=max(\\left| X_j \\right|)^{\\alpha}/max(\\left| W_j \\right|)^{1-\\alpha}")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.7167em",verticalAlign:"-0.2861em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span)))))),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}}),l.createElement(a.span,{className:"mrel"},"="),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}})),l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"1.1502em",verticalAlign:"-0.2861em"}}),l.createElement(a.span,{className:"mord mathnormal"},"ma"),l.createElement(a.span,{className:"mord mathnormal"},"x"),l.createElement(a.span,{className:"mopen"},"("),l.createElement(a.span,{className:"minner"},l.createElement(a.span,{className:"mopen delimcenter",style:{top:"0em"}},"∣"),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.07847em"}},"X"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"-0.0785em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span)))))),l.createElement(a.span,{className:"mclose delimcenter",style:{top:"0em"}},"∣")),l.createElement(a.span,{className:"mclose"},l.createElement(a.span,{className:"mclose"},")"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.7144em"}},l.createElement(a.span,{style:{top:"-3.113em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.0037em"}},"α"))))))))),l.createElement(a.span,{className:"mord"},"/"),l.createElement(a.span,{className:"mord mathnormal"},"ma"),l.createElement(a.span,{className:"mord mathnormal"},"x"),l.createElement(a.span,{className:"mopen"},"("),l.createElement(a.span,{className:"minner"},l.createElement(a.span,{className:"mopen delimcenter",style:{top:"0em"}},"∣"),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"}},"W"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.3117em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"-0.1389em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.05724em"}},"j")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.2861em"}},l.createElement(a.span)))))),l.createElement(a.span,{className:"mclose delimcenter",style:{top:"0em"}},"∣")),l.createElement(a.span,{className:"mclose"},l.createElement(a.span,{className:"mclose"},")"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.8641em"}},l.createElement(a.span,{style:{top:"-3.113em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mtight"},l.createElement(a.span,{className:"mord mtight"},"1"),l.createElement(a.span,{className:"mbin mtight"},"−"),l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.0037em"}},"α")))))))))))))),"\n",l.createElement(a.p,null,l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 535px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/db0998625891bd17a196749031a64e82/b5245/02.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 36.36363636363637%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAA7DAAAOwwHHb6hkAAABrUlEQVR42l2Ry2/TQBDG8/9fOcAFwaFCVIiKQ1uJkqRqKeTh2I7xI35sHNuxXNfNQxDiOInDj024MdJq5pud+b7dmQbSssdHHNtmuVgcIX8Oh5M/SP9rtWK9XlPXNXmeE4Yhvu8Rx5HM7fnfGicCefbystpWp+SxeVbkpPEEVemiawOeJa42FavfJfPlT2bzObtdzUoKHoWKomA+X9A4viJNY4ZXN+jNe/LgjkfRwfqhEIwUPFvB0DtEY0eSLAiMNm7vHK3XIgm+kYYGQr3EH1wSmA80sixD0/rY717gtj6RfH9NcPsWz3cQgcHIUbFMlSQZUzzleFoToV0Tu1+ZWBdM7TZZ75Xse8nUuKIRRxG6PiBsf5SFd0y6F2TqDYFjIvwhnqlg6QpR6DF7LiTuEthdxKhD4kmyxCd0HhDWPcJVaVRVRRDY9M4+0P/cJFPOKdRrLNfEsvqYZh9XxpOJz1LOLrK+EA/fM/Y7pH6LPFIQgzdExpkUuP23lPpQsytL9tst9a6k3m7YyvhJfrEs1+z3OzabUhIuiaNQjsJFCJ+x8JhOY7I0Ihy7p8X9BTaY+SpERuwPAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="Figure 5"\n        title=""\n        src="/static/db0998625891bd17a196749031a64e82/b5245/02.png"\n        srcset="/static/db0998625891bd17a196749031a64e82/06437/02.png 176w,\n/static/db0998625891bd17a196749031a64e82/ba1c3/02.png 352w,\n/static/db0998625891bd17a196749031a64e82/b5245/02.png 535w"\n        sizes="(max-width: 535px) 100vw, 535px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}}),l.createElement(a.br),"\n",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.mi,null,"α")),l.createElement(a.annotation,{encoding:"application/x-tex"},"\\alpha")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.4306em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"}},"α"))))),"를 큰 값으로 설정하여 quantization difficulty를 weight에 더 부담하는 등 hyper-parameter의 조절을 통해 quantization difficulty를 조절할 수 있다.\r\n대부분의 모델에서 ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.mi,null,"α"),l.createElement(a.mo,null,"="),l.createElement(a.mn,null,"0.5")),l.createElement(a.annotation,{encoding:"application/x-tex"},"\\alpha=0.5")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.4306em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"}},"α"),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}}),l.createElement(a.span,{className:"mrel"},"="),l.createElement(a.span,{className:"mspace",style:{marginRight:"0.2778em"}})),l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.6444em"}}),l.createElement(a.span,{className:"mord"},"0.5"))))),"에서 균형을 갖춘 quantization difficulty를 확인할 수 있었다."),"\n",l.createElement(a.p,null,"기본적으로 self-attention과 feed-forward layer에 대해 smoothing을 적용하였다.\r\nReLU, Softmax, 그리고 Layer-Norm과 같은 연산은 FP16으로 유지하였고,\r\n무거운 연산을 하는 linear layer와 BMM(Batched Matrix Multiplication)에 대해 W8A8(weight 8 actication 8) quantization을 적용하였다.\r\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 513px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/85ec202d4024106aa2c583828fbb1927/267f6/03.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 63.63636363636363%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADQElEQVR42j2T2U8bVxTG/bf0rX3qU/+QSlWlPlSqmqqqlKKkEKzUbMGgJE0hqFXTqn1IWggQCGUxZbBZzJJAoCwB4y2BGuyxPbbH4/EGZrz8csduckdHOldndO73ne87loSioIR9xCM+ouEA8pmf+NkBsdAWKTWFkkiSSKn1PJ1WUVMpMprGyfExAb8fTRW1WIiTrb+IxWJYMhmNaimBeQqhUaIz7+F98iH7gx/gefIRyfn3KcrOep1ahVqtVk9lWcbnC1C60MlnI2yv96GKRy26aFi5SJG7LLHj/Zu99XY2VrpxzDWxvWZn/1k7L4MS50aZasWgcnlOxSgihwVC7wGZvIaSVRjfna6jt+RzWXTtlNvzdq4ON2Gbu02b1Mv1pzZsUg/fTbVzZehL7jp7OddjlEs5ytUKVRNwtSzuWd6edDqNJZfVuSwoSIcz/Lx4j7GNQUae/cmD5T4ce+O8jvvZPH5OKPUfhvjPJJzcvcOp9AmHE5/hefoxrxyfoqx+TfQs2GhYyisMrP+GbcFOy1ALLcNWbC4799d+pVqtEdfjVGtVysVkvWEu5CT0vI8d1w+sz3Xyam0A3T9MMh7Bks1mMIoJErv9nLqbOZi5imf2GuGVZhI7/fWGMUG1IugZ5w3xpo/mubl4C+ukTYzESttyN5MeSYwuazZsUE7s3EF2f8va+BdsTHxFdLUJ+YWdQrFAMBbEKF8K8VQMw2DJ48SxP8nMi1FmNx6KfIqlQwlFiTcon+fi9C4PcE1q43t3NzccnXzzyMpNyY7N2cqVsc/5yd0PQgC9oDPg/hH7fCetg81YR67T4+rivvseJ2cnAqGeqdvmRAzfG95lYusxv0h99Iy18cDZz+TuBP6oj7B6RrWO8JKlIxezninGt0f5Y+V3/vE4WPS6BEJF+FA3ja2+k35w8xE3hpppF8K0jrQw+u9ww9PiM4pKPV/0LdDl6qBruoOOsVY65204vRJZc4amu/PqMXktIiIsIoquHPFy4S459VTc5f9rEfSEl3xGFswV4ckk8YCL4OZDMV+E2ctiTRVTlCyapor9bIS5OZqqEH69h55OiXv6Xe1taFoKU8yEHCASXCVXuMBcYV3XeQMYTnigp5Xv7gAAAABJRU5ErkJggg==\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="Figure 6"\n        title=""\n        src="/static/85ec202d4024106aa2c583828fbb1927/267f6/03.png"\n        srcset="/static/85ec202d4024106aa2c583828fbb1927/06437/03.png 176w,\n/static/85ec202d4024106aa2c583828fbb1927/ba1c3/03.png 352w,\n/static/85ec202d4024106aa2c583828fbb1927/267f6/03.png 513w"\n        sizes="(max-width: 513px) 100vw, 513px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}})),"\n",l.createElement(s.Z,null,l.createElement(a.p,null,"그럼 caplibration sample data가 많을 수록 더 정확한 scale값을 추정할 수 있다고 말할 수 있을까?\r\n구현을 sample data에 대한 activation의 max값을 이용하여 scaling을 적용하는데, 몇몇 특수 케이스가 해당 max값을 너무 높여서 꼭 데이터가 많다고 좋아지는 것은 아닌 것 같았다.")),"\n",l.createElement(a.h1,{id:"implement"},"Implement"),"\n",l.createElement(a.p,null,"위 논문을 바탕으로 구현을 시작해 봅시다."),"\n",l.createElement(a.h2,{id:"fakequant"},"FakeQuant"),"\n",l.createElement(a.p,null,"pytorch는 실제로 int8 연산을 지원하지 않기 때문에 float32 연산이지만 quantization을 하는 것처럼 시뮬레이션하는 fake quantization을 만들어 볼게요.\r\nquantization을 어떤 범위로 하느냐에 따라 per-tensor, per-token, per-channel 등 여러 종류가 있지만 per-tensor로 구현을 해봅시다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"@torch.no_grad()\r\ndef quantize_symmetric_per_tensor(tensor:torch.Tensor, n_bits:int):\r\n    t_max = torch.max(torch.abs(tensor))\r\n    q_max = 2**(n_bits - 1) - 1\r\n    scale = t_max.clamp(EPS).div(q_max)\r\n    q_tensor = tensor.div(scale).round()\r\n    return q_tensor, scale, torch.tensor(0)\n")),"\n",l.createElement(a.p,null,"quantization시 텐서가 가지고 있는 규모를 n bit에 나눠서 분할하기 때문에 크기를 조절해주는 scale을 구합니다.\r\n이를 이용해서 tensor를 quantization 해보았어요.",l.createElement(a.br),"\n","그럼 matmul 연산을 이용하여 테스트를 해 볼까요?"),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"def matmul_with_quantize(t1, t2, quantize):\r\n    q1, s1, z1 = quantize(t1, 8)\r\n    q2, s2, z2 = quantize(t2, 8)\r\n    return torch.matmul(q1+z1, (q2+z2).T) * s1 * s2\r\n\r\ntorch.manual_seed(42)\r\nt1 = torch.rand(128, 768) * 2 - 1\r\nt2 = torch.rand(128, 768) * 2 - 1\r\nmatmul_result = torch.matmul(t1, t2.T)\r\nq_matmul_result = matmul_with_quantize(t1, t2, quantize_symmetric_per_tensor)\r\nprint('MSE loss', F.mse_loss(matmul_result, q_matmul_result))\n")),"\n",l.createElement(a.pre,null,l.createElement(a.code,null,"MSE loss tensor(0.0027)\n")),"\n",l.createElement(a.p,null,"quantized된 두 tensor를 이용하여 연산을 진행한 후 원래 크기로 돌려주기 위해 scale값을 곱해줍니다.\r\n평균 제곱 오차를 구하면 약 0.0027 정도가 되네요."),"\n",l.createElement(a.p,null,"위 방법은 0을 기준으로 양수와 음수를 같은 규모로 양자화를 적용하는 symmetric quantization 이에요.\r\n만약에 tensor가 0을 기준으로 양수 혹은 음수에 조금 더 많이 분포하는 경향이 있으면 이 방법은 비효율적이죠.\r\n아래 예시를 보겠습니다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"t1 = t1 + 10\r\nt2 = t2 - 10\r\nq_matmul_result = matmul_with_quantize(t1, t2, quantize_symmetric_per_tensor)\r\nprint('MSE loss', F.mse_loss(matmul_result, q_matmul_result))\n")),"\n",l.createElement(a.pre,null,l.createElement(a.code,null,"MSE loss tensor(84.2358)\n")),"\n",l.createElement(a.p,null,"극단적으로 t1은 전부 양수값을 가지도록, t2는 전부 음수값을 가지도록 설정해보았어요.\r\n그 후 symmetric quantization을 진행하였는데 기존 값과 오차가 훨씬 큰 것을 볼 수 있어요."),"\n",l.createElement(a.p,null,"이런 친구들을 위해 0 기준 양쪽에 bit를 동등하게 할당하지 않고 tensor의 최대 최소값을 구한 후 중앙값을 0으로 만들어주는 전략을 취한다면?\r\n사실 앞서 quantize 함수에서 quantized된 tensor와 scale 값, 그리고 또 하나의 값을 반환하였어요.\r\n이것이 중앙값을 0으로 만들어주는 일종의 offset 개념인 zero point 입니다.\r\n우리는 zero point를 이용하여 asymmetric quantizatio을 만들어 봅시다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"@torch.no_grad()\r\ndef quantize_asymmetric_per_tensor(tensor:torch.Tensor, n_bits:int):\r\n    t_max = torch.max(tensor)\r\n    t_min = torch.min(tensor)\r\n    scale = (t_max - t_min) / (2**n_bits - 1)\r\n    zero_point = torch.round((t_max + t_min)/ 2 / scale)\r\n    q_tensor = torch.round(tensor / scale) - zero_point\r\n    return q_tensor, scale, zero_point\n")),"\n",l.createElement(a.p,null,"tensor의 최대 최소값의 범위만큼 bit를 할당하도록 scale값과 할당된 bit의 중앙값을 설정하는 zero point를 구하였습니다.\r\n이를 사용하여 아까 편향된 텐서를 계산해서 기존 결과와 얼마나 차이가 나는지 테스트 해 보겠습니다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"q_matmul_result = matmul_with_quantize(t1, t2, quantize_asymmetric_per_tensor)\r\nprint('MSE loss', F.mse_loss(matmul_result, q_matmul_result))\n")),"\n",l.createElement(a.pre,null,l.createElement(a.code,null,"MSE loss tensor(0.6758)\n")),"\n",l.createElement(a.p,null,"symmetric quantization 방법과 비교하면 loss가 크게 감소한 것을 볼 수 있네요.\r\n이렇게 보면 asymmetric quantization이 무조건 좋은 방법처럼 보이지만 symmetric에 비해 추가적인 연산을 해야 하는 부분이 있죠.\r\n그래서 두 가지 방법을 적절하게 사용하는 능력이 중요할 것 같네요."),"\n",l.createElement(s.Z,null,l.createElement(a.p,null,"구현을 진행하며 알게 된 사실인데 float16 연산에 대해 clamp와 round 함수에 오류가 발생햇다.\r\n알고보니 float16에 대한 해당 함수들은 cpu에서 불가능하고 cuda에서만 가능하였다. 허허 참..")),"\n",l.createElement(a.p,null,"위에서 만든 함수를 이용하여 linear 레이어의 weight와 bias, 그리고 input에 대해 quantization을 진행하는 새로운 레이어를 만들어 볼게요.\r\n먼저 기존 레이어의 weight는 이미 알고 있는 값이기에 이를 quantize한 값과 ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"s"),l.createElement(a.mi,null,"w"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"s_w")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.1514em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02691em"}},"w")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.15em"}},l.createElement(a.span)))))))))),", ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"z"),l.createElement(a.mi,null,"w"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"z_w")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.04398em"}},"z"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.1514em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"-0.044em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02691em"}},"w")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.15em"}},l.createElement(a.span)))))))))),"를 만들거예요.\r\nLLM과 같은 모델에서 미리 어떤 scale 값으로 quantization을 진행할지 정해두는 static quantization 보다,\r\ninput에 대해 동적으로 quantization을 진행하는 dynamic quantization이 더 효율이 좋다고 합니다.\r\n우리는 LLM을 타겟으로 하기 때문에 input을 동적으로 quantization을 진행하고 ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"s"),l.createElement(a.mi,null,"x"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"s_x")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.1514em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight"},"x")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.15em"}},l.createElement(a.span)))))))))),"와 미리 계산해 둔 ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.msub,null,l.createElement(a.mi,null,"s"),l.createElement(a.mi,null,"w"))),l.createElement(a.annotation,{encoding:"application/x-tex"},"s_w")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),l.createElement(a.span,{className:"mord"},l.createElement(a.span,{className:"mord mathnormal"},"s"),l.createElement(a.span,{className:"msupsub"},l.createElement(a.span,{className:"vlist-t vlist-t2"},l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.1514em"}},l.createElement(a.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},l.createElement(a.span,{className:"pstrut",style:{height:"2.7em"}}),l.createElement(a.span,{className:"sizing reset-size6 size3 mtight"},l.createElement(a.span,{className:"mord mathnormal mtight",style:{marginRight:"0.02691em"}},"w")))),l.createElement(a.span,{className:"vlist-s"},"​")),l.createElement(a.span,{className:"vlist-r"},l.createElement(a.span,{className:"vlist",style:{height:"0.15em"}},l.createElement(a.span)))))))))),"를 이용하여 bias 역시 quantization을 진행하도록 하죠.\r\n즉, weight만이 offline으로 계산되었고, 나머지는 동적으로 계산되는 형태를 가지게 될 것입니다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"class FakeQuantLinear(nn.Module):\r\n    def __init__(self, weight:torch.Tensor, bias:torch.Tensor, n_bits:int, quantize=quantize_symmetric_per_tensor) -> None:\r\n        super().__init__()\r\n\r\n        self.quantize = partial(quantize, n_bits=n_bits)\r\n        q_w, s_w, z_w = self.quantize(weight)\r\n        self.register_buffer('weight', q_w.squeeze(0))\r\n        self.register_buffer('s_w', s_w)\r\n        self.register_buffer('z_w', z_w)\r\n        self.register_buffer(\"bias\", bias)\r\n        \r\n    def forward(self, x):\r\n        q_x, s_x, z_x = self.quantize(x)\r\n        scale = s_x * self.s_w\r\n        bias = self.bias if self.bias is None else self.bias.div(scale).round()\r\n        out = F.linear(q_x.sub(z_x), self.weight.sub(self.z_w), bias)\r\n        return out.mul_(scale)\n")),"\n",l.createElement(a.p,null,"이를 적용해서 기존의 linear 레이어와 얼마나 차이가 발생하는지 테스트를 진행해볼게요."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"linear = nn.Linear(t.size(1), 512)\r\nresult = linear(t)\r\nquant_linear = FakeQuantLinear(linear.weight.detach(), linear.bias.detach(), 8)\r\nprint('[quantized linear] MSE loss', F.mse_loss(result, quant_linear(t)).item())\n")),"\n",l.createElement(a.pre,null,l.createElement(a.code,null,"MSE loss 3.4490449252189137e-06\n")),"\n",l.createElement(a.h2,{id:"smoothquant-1"},"SmoothQuant"),"\n",l.createElement(a.p,null,"위에서 실제 quantization과 비슷하도록 시뮬레이션 할 수 있는 fake quantization을 구현하였어요.\r\n이제 이를 활용하여서 기존의 linear 레이어의 outlier를 scaling하는 smooth quantization을 구현하도록 하겠습니다."),"\n",l.createElement(a.p,null,"smooth quant를 적용하기 위해서 해당 레이어의 activation의 채널별 최대값, weight의 채널별 최대값이 필요합니다.\r\nweight는 학습을 진행하고 나면 고정되어 최대값을 얻을 수 있지만, activation은 실시간으로 다른 값이 들어오기 때문에 미리 얻을 수 없죠.\r\n그래서 sample data를 활용하여 해당 레이어의 activation 값을 어느정도 미리 조사하고 이를 활용하는 방식을 사용합니다.\r\n일단은 activation의 scale값을 가지고 있다고 생각하고 구현을 합시다."),"\n",l.createElement(a.p,null,"먼저 activation의 channel과 연산을 담당하는 weight의 out channel에 대해 최대값들을 구해줍니다.\r\n이를 activation scale값에 각각 일정 비율로 나눠어 smooth scale을 구할거예요.\r\n여기서 일정 비율이란 ",l.createElement(a.span,{className:"math math-inline"},l.createElement(a.span,{className:"katex"},l.createElement(a.span,{className:"katex-mathml"},l.createElement(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML"},l.createElement(a.semantics,null,l.createElement(a.mrow,null,l.createElement(a.mi,null,"α")),l.createElement(a.annotation,{encoding:"application/x-tex"},"\\alpha")))),l.createElement(a.span,{className:"katex-html","aria-hidden":"true"},l.createElement(a.span,{className:"base"},l.createElement(a.span,{className:"strut",style:{height:"0.4306em"}}),l.createElement(a.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"}},"α"))))),"값을 의미하는데 activation과 weight의 scaling중 어느 부분에 더 중점을 둘지 정하는 하이퍼 파라미터에요.\r\n대부분의 경우 0.5로 두어도 잘 작동한다고 합니다.\r\n위에서 구한 smooth scale을 이용하여 weight를 scale up 해주고 들어오는 input을 scale down을 해줄거에요.\r\n그리고 그 값들을 이용하여 quantization을 진행하는 것입니다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},'EPS = 1e-8\r\nclass SmoothQuantLinear(nn.Module):\r\n    def __init__(self, weight:torch.Tensor, bias:torch.Tensor, act_scale:torch.Tensor, n_bits:int, alpha:float = 0.5):\r\n        super(SmoothQuantLinear, self).__init__()\r\n        \r\n        w_abs_max = weight.abs().max(dim=0)[0].clamp(min=EPS)\r\n        scale = act_scale.pow(alpha).div(w_abs_max.pow(1 - alpha)).clamp(min=EPS)\r\n        self.register_buffer("scale", scale) # smoothing scale\r\n        self.fake_quant_linear = FakeQuantLinear(weight * scale.view(1, -1), bias, n_bits)\r\n\r\n    def smooth(self, x):\r\n        return x / self.scale\r\n    \r\n    def forward(self, x):\r\n        smooth_x = self.smooth(x)\r\n        output = self.fake_quant_linear(smooth_x)\r\n        return output\n')),"\n",l.createElement(a.p,null,"이제 기존의 linear 레이어와 fake quant linear 레이어, smooth quant linear 레이어의 오류 차이를 비교해볼게요.\r\n일단 우리가 smooth quant를 적용하는 목적인 특정 채널에 대해서 outlier가 존재하는 텐서를 만들어줍니다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"torch.manual_seed(42)\r\nt = torch.rand((1, 128, 768))\r\nt[:,:,torch.randint(0, 767, (5,))] = torch.rand((1, 128, 5)) * 10 + 50\n")),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 447px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/a8f6a1af2c3182c00e02cb4e515a7425/a2d48/04.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 89.20454545454545%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADG0lEQVR42q2U20sUURzH96+JgqAQ36yXHoIwUIKKMM00N82ovKRpFolp0VbQQ6Bp1INF0cUCK5akdcMsTcMu0Lrr3nRnZue2s7uzu7qX2W/nHHFbRauHDgxnLmc+v+/3d37nZ8J/HqaNPmQyGcTiccTJlUql2PN6I5vN/h2YSiURiUQQCoWgKAqZVWiaBl3XkUgkWADDMFZBV8CmtdHoD+FwmEBkBBUfgrIPqioRsEpmhcwyC0QDxGIxFnhdhdQSVUAXKiEBguqAS7DDL04QSACqEoKs8uRezFMewvz8PBOQTCZ/AyksGo1CJapUjQcnf4UraMUMNwinMAxZ8UJSghBkJxRVICCNqFUZkOM4yLLMxFCHDJhOp5kySfUgoHzBLIHMcP0Yn7fgG3cfvPgdvOTGgjRNgDyBLQPpJQgCU+j1eiGK4gowQ4AqeGUGs+JLAuvFxEIP3vlaMRm4iWB0Enz4J/zSGCSZ2tdYPleAVCG9pznNWdbCGrH6icD6COwy7P4m3Bvdh/4XlRgauo2Pn61w8q8RSwYRX1xEOEKgoWWgJEls5/NyaBAgSbBswxR/HSPuGtx9VYIbd0rRWFuD6r21OFNhhqWrAQM9D2EfnoLX6YckSjmFNG05IK0pTQshoI3A5mxG3/NiXLGUoflIPepKT+NU5Qm0nTyOqh2NOLC5BQe3nkVD8TXMTP5AJBpmVlfVIbVM68k2dgtdV0vQdakMtXvaYN7VisZjdWiqr0dVUQvKt51HRSF5v7MT1qd2Zpvu9ErJ5ICULooCJsfH8HhwABcqu3Foy0XU7CYwsxlHi9pRUdCB8oJzqC7qxNuhMSSSOvw+P1wu16pTY1o+ail2ZunQY3FwC0F8sE7hQe8g2sssOLy9A/s3NeBoYQesz94jsaST/Mnw+Xy5zVgFpJYNI8Pqiec5FjG+qCND3tEjNzE6jYHuJ3jzyEaUxdgmeDweOBwOlr/8JmFa2zXo4rm5ObjdHhZIEHgYWYPA09ATOss1LeB8V3/sNktLS0Qlz8D0J1YSqTQJ4mYAqig/Z//cD9d2obUtayPgL4AaF3aVn/HkAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="04"\n        title=""\n        src="/static/a8f6a1af2c3182c00e02cb4e515a7425/a2d48/04.png"\n        srcset="/static/a8f6a1af2c3182c00e02cb4e515a7425/06437/04.png 176w,\n/static/a8f6a1af2c3182c00e02cb4e515a7425/ba1c3/04.png 352w,\n/static/a8f6a1af2c3182c00e02cb4e515a7425/a2d48/04.png 447w"\n        sizes="(max-width: 447px) 100vw, 447px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}}),"\n",l.createElement(a.p,null,"위에서 생성한 텐서의 모습을 그래프로 표현한거예요.\r\n다섯개의 채널에 대해서만 높은 값을 가지는 것을 볼 수 있죠.\r\n이를 이용하여 간단한 linear 레이어와 이를 두 가지 방식으로 quantization한 레이어의 오차를 비교합시다."),"\n",l.createElement(a.pre,null,l.createElement(a.code,{className:"language-python"},"linear = nn.Linear(768, 128)\r\nresult = linear(t)\r\n\r\nfq_linear = FakeQuantLinear(linear.weight.detach(), linear.bias.detach(), 8)\r\nprint('[fake quant] MSE loss', F.mse_loss(result, fq_linear(t)).item())\r\n\r\nact_scale = torch.max(t, dim=1).values\r\nsmooth_linear = SmoothQuantLinear(linear.weight.detach(), linear.bias.detach(), act_scale, 8)\r\nprint('[smooth quant] MSE loss', F.mse_loss(result, smooth_linear(t)).item())\n")),"\n",l.createElement(a.pre,null,l.createElement(a.code,null,"[fake quant] MSE loss 0.0059794094413518906\r\n[smooth quant] MSE loss 0.0003501520841382444\n")),"\n",l.createElement(a.p,null,"확실히 smooth quant의 방식이 기존 fake quant의 방식보다 더 적은 오차를 내는 것을 확인할 수 있네요."),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 704px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/5259675a92ae237c26aa278ff2fc4332/abe2a/05.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 53.40909090909091%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACbUlEQVR42n1TW0tUURg9f6anfkB/IUiKoiQZKh8SNOkipvkSpODDqJmU1XSB8EnNLJIM88lAMRIrK52cGXJunjm3fe5nzlxdfXuPghG0YZ2zz/m+vb61v722hMMjnQZSKWBvD/8b1WoV1WQSMIzGj0P5En/U6zUEgQ+b4FcqKJdKqNVqlPc3Mf8uUcw0TTDXhe158AlhGIoiPC7xieu4UFkWspaAoWtgzBCLHMeB7/toFK2LuWVZYCYDI3UcBqM5gedzSI5jQ9GTiMvzSMgfoGm7ULQdmJSk67oAr+yREk5m2QRmUdyGvk9o0JvHVFWFpLJt/JLfYDU3hA3lGQw3AcXaElV1vaG0Xm8QmpZJu/EhG3moZhol34VHu2P7KnlxKaOtYL0QxdyPVjx92Ya3sxPYiC/Adm04ni0S+XCpZ5xMMZOIrYxicHoMM4urWE/9FCodKsaVSillDq8/RzA02oKrkSvojFxG19ke9J65h/eTSyiGgSAMqH/JwhrGlwbQ9ugOTvfF0DwwjEsDD9E+PI1Xi5/ogFxIS8sTuN3bigvHrqOjuQ3tJ27i3JEedJ0cwta3beqTJggNpmB84QHOj/Sj6VYMkcG7uNgfQ9ONJ2gfmcL3zSS1h0FizMTmegKzz+cR7R5Dy9E+dJ+KIruTh6LKsC1bEHqej1xBwce1OMZn3uHa/Rc43vEYndFJ/M7torCbA+eSuIcMU0cQerTYwdbXbaRTWbieA1mWUSFfHtiGO8KhUw6LPgqKgeUvcSRSGdG/XD4v/CiMzVVwlCtlBJRcLBbJ3OV/bggnz2Sy5ENTeDIIPHEhQjL7wU35A7ozDzC86iKxAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="05"\n        title=""\n        src="/static/5259675a92ae237c26aa278ff2fc4332/5ebd7/05.png"\n        srcset="/static/5259675a92ae237c26aa278ff2fc4332/06437/05.png 176w,\n/static/5259675a92ae237c26aa278ff2fc4332/ba1c3/05.png 352w,\n/static/5259675a92ae237c26aa278ff2fc4332/5ebd7/05.png 704w,\n/static/5259675a92ae237c26aa278ff2fc4332/fd84e/05.png 1056w,\n/static/5259675a92ae237c26aa278ff2fc4332/abe2a/05.png 1067w"\n        sizes="(max-width: 704px) 100vw, 704px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}}),"\n",l.createElement(a.p,null,"이는 위 텐서에 scale down을 적용한 모습입니다.\r\n적용하기 전보다 outlier 채널들의 값이 낮아진 것을 확인할 수 있어요."),"\n",l.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 704px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/aa6734fea9c5d8bcf9be6720cf18c766/fd84e/06.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACFElEQVR42oWSy2sUQRDG54/yIiKag8fgLV495BRB8OAh4EFhI4uQQ8Q8VFaiiyIx4oOIoBICi8YQfFwMsruSNZPM7M57pqfnsY/5rOrNasSDDU339Hz9q+qqT8ORUfT7QL0OGAb+N4o4BppNgO8cGdpo081zBGEIkWVIez30aBZF8S+IzmKChUJA0p2M5mAw+ANkQZIkCIIAruvCsW14tPKez0IKwnAevEZRBM/3hxrSuo4Dz/OUljkaR+MPzi4MQvj002HwIdSyLAXiLFjn+Xw5RBwJBbLtIbDT6ahgmqDUfRJmscS+ZcKyLXTjBFIFCOBQBiMg6xhkeW18+9kgkINURpBxpHQKKKUkgEStsYOJJwuYfLaMhfdvUKvvoLmvq+gM69OUIoZJsJmXjzBRmsP0zVXcf7WBrS91HBhtVVstT1JUt2s4vlTGycp1nKqWcaIyg2O3buDBxw2kJGIYqD+7ho6puxWMXZrF2cu3MT69iDOTc5i6+hDN3T3keUZPjgWef93GlddPcW5lHqfvzWKsWsKdD29htzvkIGPUXrRMA6vvtnBtaQ3nS8sYv7iIC+XHaPzYwwG9hsuncX0SqosMBQyrjRcEX/m0CUGd1HUdEYlGdsnJUoLqKESIlm5gbf0zvjdbCKiWptkmzWDoQ0HPMk1TdTlPUyTsMZmg2+3+5cE+mdj1fNX5lCySpsnhmv724i/Ol9qZcUGZrgAAAABJRU5ErkJggg==\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="06"\n        title=""\n        src="/static/aa6734fea9c5d8bcf9be6720cf18c766/5ebd7/06.png"\n        srcset="/static/aa6734fea9c5d8bcf9be6720cf18c766/06437/06.png 176w,\n/static/aa6734fea9c5d8bcf9be6720cf18c766/ba1c3/06.png 352w,\n/static/aa6734fea9c5d8bcf9be6720cf18c766/5ebd7/06.png 704w,\n/static/aa6734fea9c5d8bcf9be6720cf18c766/fd84e/06.png 1056w"\n        sizes="(max-width: 704px) 100vw, 704px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}}),"\n",l.createElement(a.p,null,"물론 계산의 동등함을 유지하기 위해 weight에 scale up을 적용하여 위 채널과 연산에 관여하는 부분이 높아진 것을 확인할 수 있어요.\r\n하지만 이렇게 weight 범위가 늘어났다고 하여도, acitvation의 범위 감소율이 더 크기 때문에 전체 오차는 줄어들 수 있었습니다."))}var r=function(e){void 0===e&&(e={});const{wrapper:a}=Object.assign({},(0,n.ah)(),e.components);return a?l.createElement(a,e,l.createElement(m,e)):m(e)},c=t(4316),i=t(1840),o=t(7821),p=t(2654),E=t(4111),h=t(2726),g=t(4480),u=t(2818),d=t(9213),N=t(7213),y=t(9265),b=t(9601),f=t(3071),w=t(6097),A=t(6782),v=t(4891),x=t(3387),z=t(917);const k=e=>{let{data:{mdx:a,file:t},children:s}=e;const m=(0,g.sJ)((0,u.cp)(u.eE,!1)),r=(0,g.sJ)((0,u.cp)(u.rf,!1)),c=(0,g.Zl)((0,u.cp)(u.Cy,a.tableOfContents.items));return(0,l.useEffect)((()=>{c(a.tableOfContents.items)}),[a]),(0,z.tZ)(i.Z,null,(0,z.tZ)(q,null,(0,z.tZ)(E.Z,null)),(0,z.tZ)(M,null,(0,z.tZ)(_,{className:"navigation",isNavOpened:r},(0,z.tZ)(R,{className:"hide_scroll"},(0,z.tZ)(o.Z,null))),(0,z.tZ)(S,{isNavOpened:r},(0,z.tZ)(L,{isWide:m},(0,z.tZ)(h.Z,{title:a.frontmatter.title,modifiedTime:t.modifiedTime}),(0,z.tZ)(n.Zo,{components:{p:N.Z,h1:y.H1,h2:y.H2,h3:y.H3,h4:y.H4,h5:y.H5,h6:y.H6,hr:b.Z,blockquote:f.Z,ul:A.Z,ol:w.Z,pre:v.Z,code:x.Z}},s))),(0,z.tZ)(Q,null,(0,z.tZ)(Z,null,(0,z.tZ)(p.Z,null)))))},q=(0,c.Z)("div",{target:"e1ojob7j7"})({name:"11t2x7x",styles:"display:flex;height:var(--header-height);z-index:5;padding:0.6rem 2rem 0.6rem 0.6rem;position:fixed;width:100%;background:var(--background-color);border-bottom:1px solid var(--border-color)"}),M=(0,c.Z)("div",{target:"e1ojob7j6"})({name:"majwgz",styles:"position:relative;display:flex;min-height:calc(100vh - var(--header-height));overflow-x:hidden"}),_=(0,c.Z)("aside",{target:"e1ojob7j5"})("margin-left:",(e=>e.isNavOpened?"0":"calc(-1 * var(--sidebar-width))"),";flex:0 0 var(--sidebar-width);font-size:0.875rem;overflow-x:hidden;overflow-y:auto;transition:margin 0.25s var(--ease-in-out-quad);@media (min-width: ",d.Z.IPAD_PRO,"px){margin-left:0;}"),R=(0,c.Z)("nav",{target:"e1ojob7j4"})({name:"l4vzaw",styles:"overflow-y:auto;height:100%;padding:var(--body-padding-top) 0 3rem 0;position:fixed;width:var(--sidebar-width);&:-webkit-scrollbar{display:none;}"}),L=(0,c.Z)("main",{target:"e1ojob7j3"})("padding:1rem;width:100%;@media (min-width: ",d.Z.IPAD_AIR,"px){width:",(e=>e.isWide?"90%":"65%"),";}"),S=(0,c.Z)("main",{target:"e1ojob7j2"})("width:calc(100% - 2 * var(--sidebar-width));padding-top:var(--body-padding-top);flex-grow:1;min-width:20rem;display:flex;justify-content:center;opacity:",(e=>e.isNavOpened?.3:1),";@media (min-width: ",d.Z.IPAD_PRO,"px){opacity:1;}"),Q=(0,c.Z)("aside",{target:"e1ojob7j1"})("font-size:0.75rem;font-weight:bold;overflow-x:hidden;overflow-y:auto;padding-top:var(--body-padding-top);width:0;transition:width 0.25s var(--ease-in-out-quad);@media (min-width: ",d.Z.HD,"px){width:var(--sidebar-width);}"),Z=(0,c.Z)(R,{target:"e1ojob7j0"})({name:"b40oxt",styles:"padding:0 1rem 0 1rem"});function I(e){return l.createElement(k,e,l.createElement(r,e))}}}]);
//# sourceMappingURL=component---src-components-templates-main-template-tsx-content-file-path-contents-quantization-smooth-quant-mdx-e315edf930d16a71eefa.js.map